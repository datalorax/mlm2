---
title: "Multilevel logistic regression"
subtitle: "And  more Bayesian estimation"
author: "Daniel Anderson "
date: "Week 8"
output:
  xaringan::moon_reader:
    css: ["default", "new.css"]
    lib_dir: libs
    nature:
      navigation:
        scroll: false
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "https://platform.twitter.com/widgets.js"
    includes:
      in_header: "load-feather.html"
---

```{r include = FALSE}
source(here::here("static", "slides", "slide-setup.R"))
xaringanExtra::use_clipboard()
library(tidyverse)
theme_set(theme_minimal(25))
knitr::opts_chunk$set(fig.width = 13, message = FALSE)
update_geom_defaults('path', list(size = 2))
update_geom_defaults('smooth', list(size = 2))
update_geom_defaults('point', list(size = 5))
update_geom_defaults('text', list(size = 9))
```

`r setup("w8p1")`

---
# Agenda
* Logistic regression review

* Extending to multilevel logistic regression models with **lme4**

* Fitting multilevel logistic regression models with **brms**

* Plotting **brms** fits

---
class: inverse-blue middle
# Review of logistic regression

---
# Data generating distribution
Up to this point, we've been assuming the data were generated from a normal distribution.

--

For example, we might fit a simple linear regression model to the wages data like this

$$
\begin{aligned}
  \operatorname{wages}_{i}  &\sim N \left(\widehat{y}, \sigma^2 \right) \\
  \widehat{y} &= \alpha + \beta_{1}(\operatorname{exper})
\end{aligned}
$$

--
In code

```{r }
wages <- read_csv(here::here("data", "wages.csv")) %>% 
  mutate(hourly_wage = exp(lnw))

wages_lm <- lm(hourly_wage ~ exper, data = wages)
```

---
# Graphically

```{r}
ggplot(wages, aes(exper, hourly_wage)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

--
Aside - you see why the log of wages was modeled instead?

---
# Log wages

```{r }
ggplot(wages, aes(exper, lnw)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

---
# Move to binary model
Let's split the wages data into a binary classification based on whether it's above or below the mean.

--

```{r }
wages <- wages %>% 
  mutate(
    high_wage = ifelse(
      hourly_wage > mean(hourly_wage, na.rm = TRUE), 1, 0
    )
  )

wages %>% 
  select(id, hourly_wage, high_wage)
```

---
# Plot

```{r fig.height = 5}
means <- wages %>% 
  group_by(high_wage) %>% 
  summarize(mean = mean(exper))

ggplot(wages, aes(exper, high_wage)) +
  geom_point(alpha = 0.01) +
  geom_point(aes(x = mean), data = means, 
             shape = 23, 
             fill = "cornflowerblue")
```

---
# We could fit a linear model

```{r }
m_lpm <- lm(high_wage ~ exper, data = wages)
arm::display(m_lpm)
```

--
This is referred to as a linear probability model (LPM) and they are pretty hotly contested, with proponents and detractors

---
# Plot

```{r }
ggplot(wages, aes(exper, high_wage)) +
  geom_point(alpha = 0.01) +
  geom_smooth(method = "lm", se = FALSE)
```

---
# Prediction

What if somebody has an experience of 25 years?

--

```{r }
predict(m_lpm, newdata = data.frame(exper = 25))
```

ðŸ¤¨

Our prediction goes outside the range of our data.


--
As a rule, the assumed data generating process should match the boundaries of the data.


--
Of course, you could truncate after the fact. I think that's less than ideal (see [here](https://www.alexpghayes.com/blog/consistency-and-the-linear-probability-model/) or [here](https://github.com/alexpghayes/linear-probability-model/blob/master/presentation.pdf) for discussions contrasting LPM with logistic regression).

---
# The binomial model

$$
y_i \sim \text{Binomial}(n, p_i)
$$

--
Think in terms of coin flips


--
$n$ is the number of coin flips, while $p_i$ is the probability of heads


---
# Example
Flip 1 coin 10 times

```{r }
set.seed(42)
rbinom(
  n = 10, # number of trials
  size = 1, # number of coins
  prob = 0.5 # probability of heads
)
```

--
Side note - a binomial model with `size = 1` (or $n = 1$ in equation form) is equivalent to a Bernoulli distribution


--

Flip 10 coins 1 time

```{r }
set.seed(42)
rbinom(n = 1, size = 10, prob = 0.5)
```

---
# Modeling
We now build a linear model for $p$, just like we previously built a linear model for $\mu$.


--
.center[
.realbig[
A problem
]
]

Probability is bounded $[0, 1]$ 

We need to ensure that our model respects these bounds


---
# Link functions

We solve this problem by using a *link* function

$$
\begin{aligned}
y_i &\sim \text{Binomial}(n, p_i) \\
f(p_i) &= \alpha + \beta(x_i) 
\end{aligned}
$$

--
* Instead of modeling $p_i$ directly, we model $f(p_i)$


--
* The specific $f$ is the link function


--
* Link functions map the *linear* space of the model to the *non-linear* parameters (like probability)


--
* The *log* and *logit* links are most common


---
# Binomial logistic regression
If we only have two categories, we commonly assume a binomial distribution, with a logit link.

$$
\begin{aligned}
y_i &\sim \text{Binomial}(n, p_i) \\
\text{logit}(p_i) &= \alpha + \beta(x_i) 
\end{aligned}
$$

--
where the logit link is defined by the *log-odds*

$$
\text{logit}(p_i) = \log\left[\frac{p_i}{1-p_i}\right]
$$


--
So

$$
\log\left[\frac{p_i}{1-p_i}\right] = \alpha + \beta(x_i) 
$$

---
# Inverse link

What we probably want to interpret is probability

--

We can transform the log-odds to probability by exponentiating 

$$
p_i = \frac{\exp(\alpha + \beta(x_i))}{1 + \exp(\alpha + \beta(x_i))}
$$
--
This is the logistic function, or the inverse-logit.


---
# Example logistic regression model

```{r}
m_glm <- glm(high_wage ~ exper, 
             data = wages, 
             family = binomial(link = "logit")) #<<
arm::display(m_glm)
```


---
# Coefficient interpretation
The coefficients are reported on the *log-odds* scale. Other than that, interpretation is the same.


--
For example:

The log-odds of a participant with zero years experience being in the high wage category was `r round(coef(m_glm)[1], 2)`. 

For every one year of additional experience, the log-odds of being in the high wage category increased by `r round(coef(m_glm)[2], 2)`.


---
# Note
* Outside of scientific audiences, almost nobody is going to understand the previous slide

* You *cannot* just transform the coefficients and interpret them as probabilities (because it is non-linear on the probability scale).


---
# Log odds scale

```{r fig.height = 5}
tibble(exper = 0:25) %>% 
  mutate(pred = predict(m_glm, newdata = .)) %>% 
  ggplot(aes(exper, pred)) +
  geom_line()
```

--
Perfectly straight line - change in log-odds are modeled as a linear function of experience

---
# Probability scale

```{r fig.height = 5}
tibble(exper = 0:25) %>% 
  mutate(pred = predict(m_glm, newdata = ., type = "response")) %>% 
  ggplot(aes(exper, pred)) +
  geom_line()
```

--
Our model parameters map to probability non-linearly, and it is bound to $[0, 1]$

---
# Probability predictions

Let's make the predictions from the previous slide "by hand"

--
Recall:

$$
p_i = \frac{\exp(\alpha + \beta(x_i))}{1 + \exp(\alpha + \beta(x_i))}
$$

--
And our coefficients are

```{r }
coef(m_glm)
```

---
# Intercept

$$
(p_i|\text{exper = 0}) = \frac{\exp(-1.65 + 0.25(0))}{1 + \exp(-1.65 + 0.25(0))}
$$

--
$$
(p_i|\text{exper = 0}) = \frac{\exp(-1.65)}{1 + \exp(-1.65)}
$$

--
$$
(p_i|\text{exper = 0}) = \frac{0.19}{1.19} = 0.16
$$

---
# Five years experience

Notice the exponentiation happens *after* adding the coefficients together

$$
(p_i|\text{exper = 5}) = \frac{\exp(-1.65 + 0.25(5))}{1 + \exp(-1.65 + 0.25(5))}
$$

--
$$
(p_i|\text{exper = 5}) = \frac{\exp(-0.4)}{1 + \exp(-0.4)}
$$

--
$$
(p_i|\text{exper = 5}) = \frac{0.67}{1.67} = 0.40
$$

---
# Fifteen years experience

$$
(p_i|\text{exper = 15}) = \frac{\exp(-1.65 + 0.25(15))}{1 + \exp(-1.65 + 0.25(15))}
$$

--
$$
(p_i|\text{exper = 15}) = \frac{\exp(2.1)}{1 + \exp(2.1)}
$$

--
$$
(p_i|\text{exper = 15}) = \frac{8.16}{9.16} = 0.89
$$


---
# More interpretation

Let's try to make this so more people might understand.


--
First, show the logistic curve on the probability scale! Can help prevent linear interpretations


--
Discuss probabilities at different $x$ values


--
The "divide by 4" rule

---
# Divide by 4

* Logistic curve is steepest when $p_i = 0.5$

* The slope of the logistic curve (its derivative) is maximized at $\beta/4$

* Aside from the intercept, we can say that the change is *no more* than $\beta/4$ 

--
## Example

$$
\frac{0.25}{4} = 0.0625
$$

--
So, a one year increase in experience corresponds to, at most, a 6% increase in being in the high wage category


---
# Example writeup

There was a `r round(predict(m_glm, newdata = tibble(exper = 0), type = "response") * 100)` percent chance that a participant with zero years experience would be in the *high wage* category. The logistic function, which mapped years of experience to the  probability of being in the *high-wage* category, was non-linear, as shown in Figure 1. At its steepest point, a one year increase in experience corresponded with approximately a 6% increase in the probability of being in the high-wage category. For individuals with 5, 10, and 15 years of experience, the probability increased to a `r round(predict(m_glm, newdata = tibble(exper = 5), type = "response") * 100)`, `r round(predict(m_glm, newdata = tibble(exper = 10), type = "response") * 100)`, and `r round(predict(m_glm, newdata = tibble(exper = 15), type = "response") * 100)` percent chance, respectively. 

---
class: inverse-red middle
# Other distributions

---
background-image: url(img/distributions.png)
background-size: contain

.footnote[Figure from [McElreath](https://xcelab.net/rm/statistical-rethinking/)]

---
class: inverse-blue middle

# Multilevel logistic regression

---
# The data
Polling data from the 1988 election.

```{r }
polls <- rio::import(here::here("data", "polls.dta"),
                     setclass = "tbl_df")
polls
```

---
# About the data

* Collected one week before the election

* Nationally representative sample

* Should use post-stratification to control for non-response, but we'll hold off on that for now (See Gelman & Hill, Chapter 14 for more details)

---
# Baseline probability

Let's assume we want to estimate the probability that Bush will be elected.

--
We could fit a single level model like this

$$
\begin{aligned}
\operatorname{bush} &\sim \text{Binomial}\left(n = 1, \operatorname{prob}_{\operatorname{bush} = \operatorname{1}}= \hat{P}\right) \\
 \log\left[ \frac { \hat{P} }{ 1 - \hat{P} } \right] 
 &= \alpha
\end{aligned}
$$

---
# In code

Can you write the code for the previous model?

`r countdown::countdown(1)`

--

```{r}
bush_sl <- glm(bush ~ 1, 
               data = polls,
               family = binomial(link = "logit"))
```


--

```{r }
arm::display(bush_sl)
```

--
So, $p_i = \frac{\exp(0.25)}{1 + \exp(0.25)} = 0.56$

---
# State-level variability

To estimate state-level variability, we just specify a distribution for the intercept variability.

--

$$
\begin{aligned}
\operatorname{bush} &\sim \text{Binomial}\left(n = 1, \operatorname{prob}_{\operatorname{bush} = \operatorname{1}}= \hat{P}\right) \\
 \log\left[ \frac { \hat{P} }{ 1 - \hat{P} } \right] 
 &= \alpha_{j[i]} \\
    \alpha_{j}  &\sim N \left(\mu_{\alpha_{j}}, \sigma^2_{\alpha_{j}} \right)
    \text{, for state j = 1,} \dots \text{,J}
\end{aligned}
$$

--
Notice we're still specifying that the intercept variability is generated by a *normal* distribution.


--
What does this variability actually represent?


--
Variance in the log-odds

---
# Fitting the model

If we're using **{lme4}**, we just swap `lmer()` for `glmer()` and specify the family and link function.

--

```{r }
library(lme4)

m0 <- glmer(bush ~ 1 + (1|state),
           data = polls,
           family = binomial(link = "logit"))
arm::display(m0)
```

---
# Interpretation

* The average log odds of supporting bush was 0.25

* This average varied between states with a standard deviation of 0.34

---
# State-level variation

```{r }
library(broom.mixed)
m0_tidied <- tidy(m0, effects = "ran_vals", conf.int = TRUE)
m0_tidied
```

---
# Fancified Plot Code
```{r eval = FALSE}
m0_tidied %>% 
  mutate(level = forcats::fct_reorder(level, estimate),
         lean_bush = ) %>% 
  ggplot(aes(estimate, level)) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  geom_point(aes(color = estimate)) +
  geom_vline(xintercept = 0, color = "gray40", size = 3) +
  labs(x = "Log-Odds Estimate", y = "") +
  colorspace::scale_color_continuous_diverging(palette = "Blue-Red 2") +
  guides(color = "none") +
  theme(panel.grid.major.y = element_blank(),
        axis.text.y = element_blank())
```

---
# Fancified Plot
```{r echo = FALSE, fig.height = 9}
m0_tidied %>% 
  mutate(level = forcats::fct_reorder(level, estimate),
         lean_bush = ) %>% 
  ggplot(aes(estimate, level)) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  geom_point(aes(color = estimate)) +
  geom_vline(xintercept = 0, color = "gray40", size = 3) +
  labs(x = "Log-Odds Estimate", y = "") +
  colorspace::scale_color_continuous_diverging(palette = "Blue-Red 2") +
  guides(color = "none") +
  theme(panel.grid.major.y = element_blank(),
        axis.text.y = element_blank())
```

---


---
# What to estimate
* State-level variation

* Correct for non-response

* 
