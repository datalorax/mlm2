---
title: "Multilevel logistic regression"
subtitle: "And  more Bayesian estimation"
author: "Daniel Anderson "
date: "Week 8"
output:
  xaringan::moon_reader:
    css: ["default", "new.css"]
    lib_dir: libs
    nature:
      navigation:
        scroll: false
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "https://platform.twitter.com/widgets.js"
    includes:
      in_header: "load-feather.html"
---

```{r include = FALSE}
source(here::here("static", "slides", "slide-setup.R"))
xaringanExtra::use_clipboard()
library(tidyverse)
theme_set(theme_minimal(25))
knitr::opts_chunk$set(fig.width = 13, message = FALSE)
update_geom_defaults('path', list(size = 2))
update_geom_defaults('smooth', list(size = 2))
update_geom_defaults('point', list(size = 5))
update_geom_defaults('text', list(size = 9))
```

`r setup("w8p1")`

---
# Agenda
* Logistic regression review

* Extending to multilevel logistic regression models with **lme4**

* Fitting multilevel logistic regression models with **brms**

* Plotting **brms** fits

---
class: inverse-blue middle
# Review of logistic regression

---
# Data generating distribution
Up to this point, we've been assuming the data were generated from a normal distribution.

--

For example, we might fit a simple linear regression model to the wages data like this

$$
\begin{aligned}
  \operatorname{wages}_{i}  &\sim N \left(\widehat{y}, \sigma^2 \right) \\
  \widehat{y} &= \alpha + \beta_{1}(\operatorname{exper})
\end{aligned}
$$

--
In code

```{r }
wages <- read_csv(here::here("data", "wages.csv")) %>% 
  mutate(hourly_wage = exp(lnw))

wages_lm <- lm(hourly_wage ~ exper, data = wages)
```

---
# Graphically

```{r}
ggplot(wages, aes(exper, hourly_wage)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

--
Aside - you see why the log of wages was modeled instead?

---
# Log wages

```{r }
ggplot(wages, aes(exper, lnw)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

---
# Move to binary model
Let's split the wages data into a binary classification based on whether it's above or below the mean.

--

```{r }
wages <- wages %>% 
  mutate(
    high_wage = ifelse(
      hourly_wage > mean(hourly_wage, na.rm = TRUE), 1, 0
    )
  )

wages %>% 
  select(id, hourly_wage, high_wage)
```

---
# Plot

```{r fig.height = 5}
means <- wages %>% 
  group_by(high_wage) %>% 
  summarize(mean = mean(exper))

ggplot(wages, aes(exper, high_wage)) +
  geom_point(alpha = 0.01) +
  geom_point(aes(x = mean), data = means, 
             shape = 23, 
             fill = "cornflowerblue")
```

---
# We could fit a linear model

```{r }
m_lpm <- lm(high_wage ~ exper, data = wages)
arm::display(m_lpm)
```

--
This is referred to as a linear probability model (LPM) and they are pretty hotly contested, with proponents and detractors

---
# Plot

```{r }
ggplot(wages, aes(exper, high_wage)) +
  geom_point(alpha = 0.01) +
  geom_smooth(method = "lm", se = FALSE)
```

---
# Prediction

What if somebody has an experience of 25 years?

--

```{r }
predict(m_lpm, newdata = data.frame(exper = 25))
```

ðŸ¤¨

Our prediction goes outside the range of our data.


--
As a rule, the assumed data generating process should match the boundaries of the data.


--
Of course, you could truncate after the fact. I think that's less than ideal (see [here](https://www.alexpghayes.com/blog/consistency-and-the-linear-probability-model/) or [here](https://github.com/alexpghayes/linear-probability-model/blob/master/presentation.pdf) for discussions contrasting LPM with logistic regression).

---
# The binomial model

$$
y_i \sim \text{Binomial}(n, p_i)
$$

--
Think in terms of coin flips


--
$n$ is the number of coin flips, while $p_i$ is the probability of heads


---
# Example
Flip 1 coin 10 times

```{r }
set.seed(42)
rbinom(
  n = 10, # number of trials
  size = 1, # number of coins
  prob = 0.5 # probability of heads
)
```

--
Side note - a binomial model with `size = 1` (or $n = 1$ in equation form) is equivalent to a Bernoulli distribution


--

Flip 10 coins 1 time

```{r }
rbinom(n = 1, size = 10, prob = 0.5)
```

---
# Modeling
We now build a linear model for $p$, just like we previously built a linear model for $\mu$.


--
.center[
.realbig[
A problem
]
]

Probability is bounded $[0, 1]$ 

We need to ensure that our model respects these bounds


---
# Link functions

We solve this problem by using a *link* function

$$
\begin{aligned}
y_i &\sim \text{Binomial}(n, p_i) \\
f(p_i) &= \alpha + \beta(x_i) 
\end{aligned}
$$

--
* Instead of modeling $p_i$ directly, we model $f(p_i)$


--
* The specific $f$ is the link function


--
* Link functions map the *linear* space of the model to the *non-linear* parameters (like probability)


--
* The *log* and *logit* links are most common


---
# Binomial logistic regression
If we only have two categories, we commonly assume a binomial distribution, with a logit link.

$$
\begin{aligned}
y_i &\sim \text{Binomial}(n, p_i) \\
\text{logit}(p_i) &= \alpha + \beta(x_i) 
\end{aligned}
$$

--
where the logit link is defined by the *log-odds*

$$
\text{logit}(p_i) = \log\left[\frac{p_i}{1-p_i}\right]
$$


--
So

$$
\log\left[\frac{p_i}{1-p_i}\right] = \alpha + \beta(x_i) 
$$

---
# Inverse link

What we probably want to interpret is probability

--

We can transform the log-odds to probability by exponentiating 

$$
p_i = \frac{\exp(\alpha + \beta(x_i))}{1 + \exp(\alpha + \beta(x_i))}
$$
--
This is the logistic function, or the inverse-logit.


---
# Example logistic regression model

```{r}
m_glm <- glm(high_wage ~ exper, 
             data = wages, 
             family = binomial(link = "logit")) #<<
arm::display(m_glm)
```


---
# Coefficient interpretation
The coefficients are reported on the *log-odds* scale. Other than that, interpretation is the same.


--
For example:

The log-odds of a participant with zero years experience being in the high wage category was `r round(coef(m_glm)[1], 2)`. 

For every one year of additional experience, the log-odds of being in the high wage category increased by `r round(coef(m_glm)[2], 2)`.


---
# Note
* Outside of scientific audiences, almost nobody is going to understand the previous slide

* You *cannot* just transform the coefficients and interpret them as probabilities (because it is non-linear on the probability scale).


---
# Log odds scale

```{r fig.height = 5}
tibble(exper = 0:25) %>% 
  mutate(pred = predict(m_glm, newdata = .)) %>% 
  ggplot(aes(exper, pred)) +
  geom_line()
```

--
Perfectly straight line - change in log-odds are modeled as a linear function of experience

---
# Probability scale

```{r fig.height = 5}
tibble(exper = 0:25) %>% 
  mutate(pred = predict(m_glm, 
                        newdata = ., 
                        type = "response")) %>% 
  ggplot(aes(exper, pred)) +
  geom_line()
```

--
Our model parameters map to probability non-linearly, and it is bound to $[0, 1]$

---
# Probability predictions

Let's make the predictions from the previous slide "by hand"

--
Recall:

$$
p_i = \frac{\exp(\alpha + \beta(x_i))}{1 + \exp(\alpha + \beta(x_i))}
$$

--
And our coefficients are

```{r }
coef(m_glm)
```

---
# Intercept

$$
(p_i|\text{exper = 0}) = \frac{\exp(-1.65 + 0.25(0))}{1 + \exp(-1.65 + 0.25(0))}
$$

--
$$
(p_i|\text{exper = 0}) = \frac{\exp(-1.65)}{1 + \exp(-1.65)}
$$

--
$$
(p_i|\text{exper = 0}) = \frac{0.19}{1.19} = 0.16
$$

---
# Five years experience

Notice the exponentiation happens *after* adding the coefficients together

$$
(p_i|\text{exper = 5}) = \frac{\exp(-1.65 + 0.25(5))}{1 + \exp(-1.65 + 0.25(5))}
$$

--
$$
(p_i|\text{exper = 5}) = \frac{\exp(-0.4)}{1 + \exp(-0.4)}
$$

--
$$
(p_i|\text{exper = 5}) = \frac{0.67}{1.67} = 0.40
$$

---
# Fifteen years experience

$$
(p_i|\text{exper = 15}) = \frac{\exp(-1.65 + 0.25(15))}{1 + \exp(-1.65 + 0.25(15))}
$$

--
$$
(p_i|\text{exper = 15}) = \frac{\exp(2.1)}{1 + \exp(2.1)}
$$

--
$$
(p_i|\text{exper = 15}) = \frac{8.16}{9.16} = 0.89
$$


---
# More interpretation

Let's try to make this so more people might understand.


--
First, show the logistic curve on the probability scale! Can help prevent linear interpretations


--
Discuss probabilities at different $x$ values


--
The "divide by 4" rule

---
# Divide by 4

* Logistic curve is steepest when $p_i = 0.5$

* The slope of the logistic curve (its derivative) is maximized at $\beta/4$

* Aside from the intercept, we can say that the change is *no more* than $\beta/4$ 

--
## Example

$$
\frac{0.25}{4} = 0.0625
$$

--
So, a one year increase in experience corresponds to, at most, a 6% increase in being in the high wage category


---
# Example writeup

There was a `r round(predict(m_glm, newdata = tibble(exper = 0), type = "response") * 100)` percent chance that a participant with zero years experience would be in the *high wage* category. The logistic function, which mapped years of experience to the  probability of being in the *high-wage* category, was non-linear, as shown in Figure 1. At its steepest point, a one year increase in experience corresponded with approximately a 6% increase in the probability of being in the high-wage category. For individuals with 5, 10, and 15 years of experience, the probability increased to a `r round(predict(m_glm, newdata = tibble(exper = 5), type = "response") * 100)`, `r round(predict(m_glm, newdata = tibble(exper = 10), type = "response") * 100)`, and `r round(predict(m_glm, newdata = tibble(exper = 15), type = "response") * 100)` percent chance, respectively. 

---
class: inverse-red middle
# Other distributions

---
background-image: url(img/distributions.png)
background-size: contain

.footnote[Figure from [McElreath](https://xcelab.net/rm/statistical-rethinking/)]

---
class: inverse-blue middle

# Multilevel logistic regression

---
# The data
Polling data from the 1988 election.

```{r }
polls <- rio::import(here::here("data", "polls.dta"),
                     setclass = "tbl_df")
polls
```

---
# About the data

* Collected one week before the election

* Nationally representative sample

* Should use post-stratification to control for non-response, but we'll hold off on that for now (See Gelman & Hill, Chapter 14 for more details)

---
# Baseline probability

Let's assume we want to estimate the probability that Bush will be elected.


--
We could fit a single level model like this

$$
\begin{aligned}
\operatorname{bush} &\sim \text{Binomial}\left(n = 1, \operatorname{prob}_{\operatorname{bush} = \operatorname{1}}= \hat{P}\right) \\
 \log\left[ \frac { \hat{P} }{ 1 - \hat{P} } \right] 
 &= \alpha
\end{aligned}
$$

---
# In code

Can you write the code for the previous model?

`r countdown::countdown(1)`

--

```{r}
bush_sl <- glm(bush ~ 1, 
               data = polls,
               family = binomial(link = "logit"))
```


--

```{r }
arm::display(bush_sl)
```

--
So, $p_i = \frac{\exp(0.25)}{1 + \exp(0.25)} = 0.56$

---
# State-level variability

To estimate state-level variability, we just specify a distribution for the intercept variability.

--

$$
\begin{aligned}
\operatorname{bush} &\sim \text{Binomial}\left(n = 1, \operatorname{prob}_{\operatorname{bush} = \operatorname{1}}= \hat{P}\right) \\
 \log\left[ \frac { \hat{P} }{ 1 - \hat{P} } \right] 
 &= \alpha_{j[i]} \\
    \alpha_{j}  &\sim N \left(\mu_{\alpha_{j}}, \sigma^2_{\alpha_{j}} \right)
    \text{, for state j = 1,} \dots \text{,J}
\end{aligned}
$$

--
Notice we're still specifying that the intercept variability is generated by a *normal* distribution.


--
What does this variability actually represent?


--
Variance in the log-odds

---
# Fitting the model

If we're using **{lme4}**, we just swap `lmer()` for `glmer()` and specify the family and link function.

--

```{r }
library(lme4)

m0 <- glmer(bush ~ 1 + (1|state),
           data = polls,
           family = binomial(link = "logit"))
arm::display(m0)
```

---
# Interpretation

* The average log odds of supporting bush was 0.25

* This average varied between states with a standard deviation of 0.34

---
# State-level variation

```{r }
library(broom.mixed)
m0_tidied <- tidy(m0, effects = "ran_vals", conf.int = TRUE)
m0_tidied
```

---
# Fancified Plot Code
```{r eval = FALSE}
m0_tidied %>% 
  mutate(level = forcats::fct_reorder(level, estimate)) %>% 
  ggplot(aes(estimate, level)) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  geom_point(aes(color = estimate)) +
  geom_vline(xintercept = 0, color = "gray40", size = 3) +
  labs(x = "Log-Odds Estimate", y = "") +
  colorspace::scale_color_continuous_diverging(palette = "Blue-Red 2") +
  guides(color = "none") +
  theme(panel.grid.major.y = element_blank(),
        axis.text.y = element_blank())
```

---
# Fancified Plot
```{r echo = FALSE, fig.height = 9}
m0_tidied %>% 
  mutate(level = forcats::fct_reorder(level, estimate)) %>% 
  ggplot(aes(estimate, level)) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  geom_point(aes(color = estimate)) +
  geom_vline(xintercept = 0, color = "gray40", size = 3) +
  labs(x = "Log-Odds Estimate", y = "") +
  colorspace::scale_color_continuous_diverging(palette = "Blue-Red 2") +
  guides(color = "none") +
  theme(panel.grid.major.y = element_blank(),
        axis.text.y = element_blank())
```

---
# Extending the model

Let's add some predictors

* Include `age`, `female` and `black` as fixed effect predictors. 


--
You try first

`r countdown::countdown(1)`

---
```{r }
m1 <- glmer(bush ~ age + female + black + (1|state),
            data = polls,
            family = binomial(link = "logit"))
arm::display(m1)
```

---
# Varying slopes
The average probability of a respondent who was coded `black == 1` supporting Bush was

$$
\small
\begin{equation}
(p_i|\text{age} = 0, \text{female} = 0, \text{black} = 1) = \frac{\exp(0.63 + (-0.08 \times 0) + (-0.11 \times 0) + (-1.84 \times 1))}{1 + \exp(0.63 + (-0.08 \times 0) + (-0.11 \times 0) + (-1.84 \times 1))}
\end{equation}
$$

--
$$
(p_i|\text{age} = 0, \text{female} = 0, \text{black} = 1) = \frac{\exp(0.63 -1.84))}{1 + \exp(0.63  -1.84)}
$$

--
$$
(p_i|\text{age} = 0, \text{female} = 0, \text{black} = 1) = \frac{\exp(-1.21))}{1 + \exp(-1.21)}
$$

--
$$
(p_i|\text{age} = 0, \text{female} = 0, \text{black} = 1) = \frac{0.29}{1.29}
$$

--
$$
(p_i|\text{age} = 0, \text{female} = 0, \text{black} = 1) = 0.22
$$

---
# Vary by state?
You try first - try to fit a model that estimates between-state variability in the relation between individuals coded `black`, and their probability of voting for Bush.

`r countdown::countdown(1)`

--

```{r }
m2 <- glmer(bush ~ age + female + black + (black|state),
            data = polls,
            family = binomial(link = "logit"))
arm::display(m2)
```

---
# Look at random effects

```{r }
tidy(m2, effects = "ran_vals", conf.int = TRUE) %>% 
  arrange(level)
```

---
# Compute log-odds
First - compute means for each group - i.e., add the coefficients

```{r}
to_plot <- tidy(m2, effects = "ran_vals", conf.int = TRUE) %>% 
  arrange(level, term) %>% 
  group_by(level) %>% 
  mutate(estimate = cumsum(estimate)) %>% 
  ungroup()

to_plot
```

---
# Create factor level

```{r }
lev_order <- to_plot %>% 
  filter(term == "(Intercept)") %>% 
  mutate(lev = forcats::fct_reorder(level, estimate))

to_plot <- to_plot %>% 
  mutate(level = factor(level, levels = levels(lev_order$lev)))
```

---
# Plot
```{r eval = FALSE}
to_plot %>% 
  mutate(
    group = ifelse(term == "(Intercept)", "Non-Black", "Black")
  ) %>%
  ggplot(aes(estimate, level)) +
  geom_line(aes(group = level), color = "gray60") +
  geom_point(aes(color = group)) +
  geom_vline(xintercept = 0, color = "gray40", size = 3) +
  labs(x = "Log-Odds Estimate", y = "") +
  scale_color_brewer(palette = "Set2") +
  theme(panel.grid.major.y = element_blank(),
        axis.text.y = element_blank())
```

---
# Plot
```{r echo = FALSE, fig.height = 8}
to_plot %>% 
  mutate(group = ifelse(term == "(Intercept)", "Non-Black", "Black")) %>%
  ggplot(aes(estimate, level)) +
  geom_line(aes(group = level), color = "gray60", size = 1.2) +
  geom_point(aes(color = group)) +
  geom_vline(xintercept = 0, color = "gray40", size = 3) +
  labs(x = "Log-Odds Estimate", y = "") +
  scale_color_brewer(palette = "Set2") +
  theme(panel.grid.major.y = element_blank(),
        axis.text.y = element_blank())
```


---
# Probability scale

```{r eval = FALSE, fig.height = 8}
to_plot %>% 
  mutate(group = ifelse(term == "(Intercept)", "Non-Black", "Black"),
         prob = exp(estimate)/(1 + exp(estimate))) %>%
  ggplot(aes(prob, level)) +
  geom_line(aes(group = level), color = "gray60", size = 1.2) +
  geom_point(aes(color = group)) +
  geom_vline(xintercept = 0.5, color = "gray40", size = 3) +
  labs(x = "Probability Estimate", y = "") +
  xlim(0, 1) +
  scale_color_brewer(palette = "Set2") +
  theme(panel.grid.major.y = element_blank(),
        axis.text.y = element_blank())
```

---
# Probability scale

```{r echo = FALSE, fig.height = 8}
to_plot %>% 
  mutate(group = ifelse(term == "(Intercept)", "Non-Black", "Black"),
         prob = exp(estimate)/(1 + exp(estimate))) %>%
  ggplot(aes(prob, level)) +
  geom_line(aes(group = level), color = "gray60", size = 1.2) +
  geom_point(aes(color = group)) +
  geom_vline(xintercept = 0.5, color = "gray40", size = 3) +
  labs(x = "Probability Estimate", y = "") +
  xlim(0, 1) +
  scale_color_brewer(palette = "Set2") +
  theme(panel.grid.major.y = element_blank(),
        axis.text.y = element_blank())
```


---
class: inverse-blue middle

# Bayes

---
# Refit
Can you fit the same model we just fit, but with **{brms}**?

You try first

`r countdown::countdown(2)`

---
# Flat priors

```{r }
library(brms)
m2_brms <- brm(bush ~ age + female + black + (black|state),
            data = polls,
            family = bernoulli(link = "logit"),
            backend = "cmdstan",
            cores = 4)
```

---
# Model summary

```{r }
summary(m2_brms)
```

---
# Posterior predictive check

```{r }
pp_check(m2_brms, type = "bars")
```

---
# Posterior predictive check 2

```{r }
pp_check(m2_brms, type = "stat")
```

---
# New data
Lung cancer data: Patients nested in doctors

```{r }
hdp <- read_csv("https://stats.idre.ucla.edu/stat/data/hdp.csv") %>% 
  janitor::clean_names() %>% 
  select(did, tumorsize, pain, lungcapacity, age, remission)
hdp
```

---
# Predict remission
Build a model where age, lung capacity, and tumor size predict whether or not the patient was in remission. Allow the intercept to vary by the doctor ID. Fit the model using **brms**

`r countdown::countdown(3)`

--
```{r }
lc <- brm(remission ~ age*tumorsize + lungcapacity + (1|did),
          data = hdp,
          family = bernoulli(link = "logit"),
          cores = 4,
          backend = "cmdstan")
```


---
# Model summary
```{r }
summary(lc)
```

---
# Posterior predictive check
```{r }
pp_check(lc, type = "bars")
```

---
# Chains
```{r fig.height = 9}
plot(lc)
```


---
# Marginal predictions: Age
```{r }
conditional_effects(lc, "age")
```

---
# Marginal predictions: tumor size
```{r }
conditional_effects(lc, "tumorsize")
```

---
# Marginal predictions: lung capacity
```{r }
conditional_effects(lc, "lungcapacity")
```

---
# Interaction

```{r }
conditional_effects(lc, "age:tumorsize")
```

---
# Make predictions

Check the relation for tumor size

```{r }
library(tidybayes)
pred_tumor <- expand.grid(
    age = 20:80,
    lungcapacity = mean(hdp$lungcapacity),
    tumorsize = 30:120,
    did = -999
  ) %>% 
  add_fitted_draws(model = lc, n = 100,
                   allow_new_levels = TRUE)
pred_tumor
```

---
# Plot
```{r }
ggplot(pred_tumor, aes(age, .value)) +
  stat_lineribbon()
```

---
# Different plot

```{r fig.height = 5}
pred_tumor %>% 
  filter(tumorsize %in% c(30, 60, 90, 120)) %>% 
ggplot(aes(age, .value)) +
  geom_line(aes(group = .draw), alpha = 0.2) +
  facet_wrap(~tumorsize) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
```

---
# Variance by Doctor
Let's look at the relation between age and proability of remission for each of the first nine doctors.

--

```{r}
pred_age_doctor <- expand.grid(
    did = unique(hdp$did)[1:9],
    age = 20:80,
    tumorsize = mean(hdp$tumorsize),
    lungcapacity = mean(hdp$lungcapacity)
  ) %>% 
  add_fitted_draws(model = lc, n = 100)
```

---
```{r }
pred_age_doctor
```

---
```{r fig.height = 9}
ggplot(pred_age_doctor, aes(age, .value)) +
  geom_line(aes(group = .draw), alpha = 0.2) +
  facet_wrap(~did)
```

---
# Going further
* We can pull lots of different things from our model

--
* Let's start by looking at what's actually in the model


--
In this case `r_*` implies "random". These are the deviations from the average.

```{r }
get_variables(lc)
```

---
# Get all draws
Let's look at the intercept

```{r }
int <- lc %>% 
  spread_draws(b_Intercept)
int
```

---
# Plot the distribution

```{r }
ggplot(int, aes(b_Intercept)) +
  geom_histogram(fill = "#61adff",
                 color = "white") +
  geom_vline(xintercept = median(int$b_Intercept),
             color = "magenta",
             size = 2)
```

---
# Grab random effects

* The random effect name is `r_did`

* We use brackets to assign new names

```{r }
spread_draws(lc, r_did[did, term])
```

---
# Look at did distributions
First 80 doctors
```{r eval = FALSE}
dids <- spread_draws(lc, r_did[did, ]) # all terms, which is just one
dids %>% 
  filter(did %in% 1:80) %>% 
  ggplot(aes(x = r_did, y = factor(did))) +
  ggridges::geom_density_ridges(color = NA, fill = "#61adff") +
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.y = element_blank())
```


---
```{r echo = FALSE, fig.height=11}
dids <- spread_draws(lc, r_did[did, ]) # all terms, which is just one
dids %>% 
  filter(did %in% 1:80) %>% 
  ggplot(aes(x = r_did, y = factor(did))) +
  ggridges::geom_density_ridges(color = NA, fill = "#61adff") +
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.y = element_blank())
```

---
# Long format

Use `gather_draws()` to return a long format, suitable for plotting (and many other things)

```{r }
fixed_l <- lc %>% 
  gather_draws(b_Intercept, b_age, b_tumorsize, b_lungcapacity, 
               `b_age:tumorsize`)
fixed_l
```

---
# Plot the densities

```{r }
ggplot(fixed_l, aes(.value)) +
  geom_density(fill = "#61adff", alpha = 0.7, color = NA) + 
  facet_wrap(~.variable, scales = "free")
```



---
# Multiple comparisons
One of the nicest things about Bayes is that any comparison you want to make can be made without jumping through a lot of additional hoops (e.g., adjusting $\alpha$).


--
### Scenario
Imagine a **35** year old has a tumor measuring **58 millimeters** and a lung capacity rating of **0.81**.


--
What would we estimate as the odds of remission if this patient had `did == 1` versus `did == 2`?


---
# Fixed effects

Not really "fixed", but rather just average relation

```{r }
fe <- lc %>% 
  spread_draws(b_Intercept, b_age, b_tumorsize, b_lungcapacity, 
               `b_age:tumorsize`)
fe
```

---
# Data
```{r }
age <- 35
tumor_size <- 58
lung_cap <- 0.81
```

--
population-level predictions

```{r }
pop_level <- 
  fe$b_Intercept +
  (fe$b_age * age) +
  (fe$b_tumorsize * tumor_size) +
  (fe$b_lungcapacity * lung_cap) +
  (fe$`b_age:tumorsize` * (age * tumor_size))
pop_level
```

---
# Plot

```{r fig.height = 6}
pd <- tibble(population_level = pop_level) 

ggplot(pd, aes(population_level)) +
  geom_histogram(fill = "#61adff",
                 color = "white") +
  geom_vline(xintercept = median(pd$population_level),
             color = "magenta",
             size = 2)
```

---
# Add in did estimates

```{r }
did1 <- filter(dids, did == 1)
did2 <- filter(dids, did == 2)

pred_did1 <- pop_level + did1$r_did
pred_did2 <- pop_level + did2$r_did
```

---
# Distributions

```{r }
did12 <- tibble(did = rep(1:2, each = length(pred_did1)),
                pred = c(pred_did1, pred_did2))

did12_medians <- did12 %>% 
  group_by(did) %>% 
  summarize(did_median = median(pred))

did12_medians
```

---
# Plot
```{r fig.height = 6}
ggplot(did12, aes(pred)) +
  geom_histogram(fill = "#61adff",
                 color = "white") +
  geom_vline(aes(xintercept = did_median), data = did12_medians,
             color = "magenta",
             size = 2) +
  facet_wrap(~did, ncol = 1)
```

---
# Transform
Let's look at this again on the probability scale. Note I'm using `brms::inv_logit_scaled()` to make the transformation.

--
```{r eval = FALSE}
ggplot(did12, aes(inv_logit_scaled(pred))) +
  geom_histogram(fill = "#61adff",
                 color = "white") +
  geom_vline(aes(xintercept = inv_logit_scaled(did_median)), 
             data = did12_medians,
             color = "magenta",
             size = 2) +
  facet_wrap(~did, ncol = 1)
```

---
```{r echo = FALSE}
ggplot(did12, aes(inv_logit_scaled(pred))) +
  geom_histogram(fill = "#61adff",
                 color = "white") +
  geom_vline(aes(xintercept = inv_logit_scaled(did_median)), 
             data = did12_medians,
             color = "magenta",
             size = 2) +
  facet_wrap(~did, ncol = 1)
```

---
# Difference
* The difference in the probability of remission for our theoretical patient is large between the two doctors.

* The median difference in log-odds is

```{r }
diff(did12_medians$did_median)
```

so the patient is about 3.5 times **more likely** to have their cancer go into remission if they had did 2, instead of 1.

--
## How confident are we in this difference?

---
# Everything is a distribution
Just compute the difference in these distributions, and we get a new distribution, which we can use to summarize our uncertainty

--
```{r }
did12_wider <- did12 %>% 
  mutate(.chain = rep(rep(1:4, each = 1000), 2),
         .draw = rep(1:1000, 8)) %>% 
  pivot_wider(names_from = "did", values_from = "pred")

did12_wider
```

---
# Compute difference
```{r }
did12_wider <- did12_wider %>% 
  mutate(diff = `2` - `1`)

did12_wider
```

---
# Summarize

```{r }
quantile(did12_wider$diff, 
         probs = c(0.025, 0.5, 0.975))
```


---
# Plot distribution

```{r }
ggplot(did12_wider, aes(diff)) +
  geom_histogram(fill = "#61adff",
                 color = "white") +
  geom_vline(aes(xintercept = median(diff)), 
             color = "magenta",
             size = 2)
```

---
# Directionality
Let's say we want to simplify the question to directionality.


--
Is there a greater chance of remission for `did` 2 than 1?

--
```{r }
table(did12_wider$diff > 0) / 4000
```


--
The distributions are not overlapping at all - therefore, we are as certain as we can be that the odds of remission are higher with `did` 2 than 1.


---
# One more quick example
Let's do the same thing, but comparing `did` 2 and 3.

```{r }
did3 <- filter(dids, did == 3)
pred_did3 <- pop_level + did3$r_did

did23 <- did12_wider %>% 
  select(-`1`, -diff) %>% 
  mutate(`3` = pred_did3,
         diff = `3` - `2`)
did23
```

---
# Directionality

```{r }
table(did23$diff > 0) / 4000
```

So there's roughly an 87% chance that the odds of remission are higher with with `did` 3 than 2.

---
# Plot data

```{r }
pd23 <- did23 %>% 
  pivot_longer(`2`:diff, 
               names_to = "Distribution",
               values_to = "Log-Odds")
pd23
```

---
```{r }
ggplot(pd23, aes(`Log-Odds`)) +
  geom_histogram(fill = "#61adff",
                 color = "white") +
  facet_wrap(~Distribution, ncol = 1)
```

---
class: inverse-green middle
# Next time
## Growth Modeling 2
We'll continue to discuss and use Bayes
