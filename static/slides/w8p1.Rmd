---
title: "More Bayes"
subtitle: "And multilevel binomial logistic regression"
author: "Daniel Anderson "
date: "Week 8"
output:
  xaringan::moon_reader:
    css: ["default", "new.css"]
    lib_dir: libs
    nature:
      navigation:
        scroll: false
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "https://platform.twitter.com/widgets.js"
    includes:
      in_header: "load-feather.html"
---

```{r include = FALSE}
source(here::here("static", "slides", "slide-setup.R"))
xaringanExtra::use_clipboard()
library(tidyverse)
theme_set(theme_minimal(25))
knitr::opts_chunk$set(fig.width = 13, message = FALSE)
update_geom_defaults('path', list(size = 2))
update_geom_defaults('smooth', list(size = 2))
update_geom_defaults('point', list(size = 5))
update_geom_defaults('text', list(size = 9))
```

`r setup("w8p1")`

---
# Agenda
* More equation practice

* Finishing up our intro to Bayes
  + Specific focus on MCMC

* Implementation with **{brms}**

* Logistic regression review

* Extending to multilevel logistic regression models with **lme4** 

---
class: inverse-blue middle
# Equation practice

---
# The data
Read in the following data

```{r message = FALSE}
library(tidyverse)
library(lme4)
d <- read_csv(here::here("data", "three-lev.csv"))
d
```

---
# Model 1

Fit the following model

`r countdown::countdown(1, 30)`

```{r echo = FALSE}
library(equatiomatic)
m1 <- lmer(math ~ mobility + (1|schid), data = d)
extract_eq(m1)
```


--

```{r eval = FALSE}
lmer(math ~ mobility + (1|schid), data = d)
```


---
# Model 2

Fit the following model

`r countdown::countdown(1, 30)`

```{r echo = FALSE}
m2 <- lmer(math ~ year + female + mobility + 
             (year|sid) +  (1|schid), 
           data = d)
extract_eq(m2)
```


--

```{r eval = FALSE}
lmer(math ~ year + female + mobility + 
             (year|sid) +  (1|schid), 
     data = d)
```

---
# Model 3

Fit the following model

`r countdown::countdown(1, 30)`

```{r echo = FALSE}
m3 <- lmer(math ~ year * black + year * hispanic + female + mobility + 
             (year|sid) +  (1|schid), 
           data = d)
extract_eq(m3, font_size = "small")
```


--

```{r eval = FALSE}
lmer(math ~ year * black + year * hispanic + female + mobility + 
             (year|sid) +  (1|schid), 
     data = d)
```

---
# Model 4

Fit the following model

`r countdown::countdown(1, 30)`

```{r echo = FALSE}
m4 <- lmer(math ~ year * female + year * mobility + lowinc +
             (year|sid) +  (year + female|schid), 
           data = d)
extract_eq(m4, font_size = "small")
```


--

```{r eval = FALSE}
lmer(math ~ year * female + year * mobility + lowinc +
             (year|sid) +  (year + female|schid), 
     data = d)
```

---
# Model 5

Fit the following model

Don't worry if you run into convergene warnings

`r countdown::countdown(1, 30)`

```{r echo = FALSE, warning = FALSE}
m5 <- lmer(math ~ year * female + mobility + lowinc +
             (year|sid) +  (year * female||schid), 
           data = d)
extract_eq(m5, font_size = "scriptsize")
```

--

```{r eval = FALSE}
lmer(math ~ year * female + mobility + lowinc +
       (year|sid) +  (year * female||schid), 
     data = d)
```

---
# Model 6

Fit the following model

`r countdown::countdown(1, 30)`

```{r echo = FALSE, warning = FALSE}
m6 <- lmer(math ~ year + female + mobility * size +
             (year|sid) +  (0 + mobility|schid), 
           data = d)
extract_eq(m6)
```


--

```{r eval = FALSE}
lmer(math ~ year + female + mobility * size +
             (year|sid) +  (0 + mobility|schid), 
     data = d)
```

---
# Model 7

Fit the following model

`r countdown::countdown(1, 30)`

```{r echo = FALSE, warning = FALSE}
m7 <- lmer(math ~ year + female + mobility + size +
             (year|sid), 
           data = d)
extract_eq(m7, font_size = "scriptsize")
```


--

```{r eval = FALSE}
lmer(math ~ year + female + mobility + size +
       (year|sid), 
     data = d)
```


---
# Model 8

Last one

`r countdown::countdown(1, 30)`

```{r echo = FALSE, warning = FALSE}
m8 <- lmer(math ~ year * lowinc + female + 
             (year|sid) + (year |schid), 
           data = d)
extract_eq(m8, font_size = "scriptsize")
```


--

```{r eval = FALSE}
lmer(math ~ year * lowinc + female + 
       (year|sid) + (year |schid), 
     data = d)
```

---
class: inverse-blue middle
# Finishing up w/Bayes intro

---
# Updated beliefs
* Remember our posterior is defined by


$$
\text{posterior} = \frac{ \text{likelihood} \times \text{prior}}{\text{average likelihood}}
$$

--
When we just had a single parameter to estimate, $\mu$, this was tractable with grid search.


--
With even simple linear regression, however, we have three parameters: $\alpha$, $\beta$, and $\sigma$


--
Our Bayesian model then becomes considerably more complicated:

$$
P(\alpha, \beta, \sigma \mid x) = \frac{ P(x \mid \alpha, \beta, \sigma) \, P(\alpha, \beta, \sigma)}{\iiint \, P(x \mid \alpha, \beta, \sigma) \, P(\alpha, \beta, \sigma) \,d\alpha \,d\beta \,d\sigma}
$$


---
# Estimation
Rather than trying to compute the integrals, we draw *samples* from the joint posterior distribution.


--
This sounds a bit like magic - how do we do this?


--
Multiple different algorithms, but all use some form of Markov-Chain Monte-Carlo sampling

---
# Conceptually

* Imagine the posterior as a hill

* We start with the parameters set to random numbers

  + Estimate the posterior with this values (our spot on the hill)

* Try to "walk" around in a way that we get a complete "picture" of the hill from the samples

* Use these samples as your posterior distribution

---
# An example

.footnote[Example from [Ravenzwaaij et al.](https://link.springer.com/article/10.3758/s13423-016-1015-8)]

Let's go back to an example where we're trying to estimate the mean with a know standard deviation.

--
$$
N(\mu, 15)
$$

--
Start with an initial guess, say 110


---
# MCMC
One algorithm:

* Generate a *proposal* by taking a sample from your best guess according to some distribution, e.g., $\text{proposal} \sim N(110, 10)$. Suppose we get a value of 108


--
* Observe the actual data. Let's say we have only one point: 100


--

* Compare the "height" of the proposal distribution to height of the current distribution, relative to observed data: $N(100|110, 15)$, $N(100|108, 15)$


---
# In code

```{r }
dnorm(110, 100, 15)
dnorm(108, 100, 15)
```

---
* If probability of target distribution is higher, accept

* If probability  is lower, randomly select between the two with probability equal to the "heights" (probability of the two distributions)

--

* If proposal is accepted - it's the next sample in the chain

* Otherwise, the next sample is a copy of current sample


--
## This completes one iteration

Next iteration starts by generating a new proposal distribution


--
Stop when you have enough samples to have an adequate "picture"

---
# Example in code
Let's take 5000 samples

```{r}
set.seed(42) # for reproducibility

samples <- c(
  110, # initial guess
  rep(NA, 4999) # space for subsequent samples to fill in
)
```

---
# Loop

```{r}
for(i in 2:5000) {
  # generate proposal distribution
  proposal <- rnorm(1, mean = samples[i - 1], sd = 10)
  
  # calculate current/proposal distribution likelihoood
  prob_current <- dnorm(samples[i - 1], 100, 15)
  prob_proposal <- dnorm(proposal, 100, 15)
  
  # compute the probability ratio
  prob_ratio <- prob_proposal / prob_current
  
  # Determine which to select
  if(prob_ratio > runif(1)) {
    samples[i] <- proposal # accept
  } else {
    samples[i] <- samples[i - 1] # reject
  }
}
```

---
# Plot
Iteration history
```{r fig.height = 5}
tibble(iteration = 1:5000,
       value = samples) %>% 
  ggplot(aes(iteration, value)) +
  geom_line() +
  geom_hline(yintercept = 100, 
             color = "magenta",
             size = 3)
```

---
# Plot
Density

```{r fig.height = 5}
tibble(iteration = 1:5000,
       value = samples) %>% 
  ggplot(aes(value)) +
  geom_density(fill = "#00b4f5",
               alpha = 0.7) +
  geom_vline(xintercept = 100, 
             color = "magenta",
             size = 3)
```

---
class: inverse-red middle
# Bayes Update Script
Last week I promised a script to let you play around with priors. I'll show that now ("bayes-update-plotting.R")

There's also a MH script I'll quickly show.

### [demo]


---
class: inverse-blue middle

# Implementation with {brms}

Luckily, we don't have to program the MCMC algorithm ourselves

.center[
![](https://github.com/paul-buerkner/brms/raw/master/man/figures/brms.png)
]

---
# What is it?
* **b**ayesian **r**egression **m**odeling with **s**tan

* Uses [stan](https://mc-stan.org/) as the model backend - basically writes the model code for you then sends it to stan

* Allows model syntax similar to **lme4** 

* Simple specification of priors - defaults are flat

* Provides many methods for post-model fitting inference

---
# Fit a basic model
Let's start with the default (uninformative) priors, and fit a standard, simple-linear regression model

```{r }
library(brms)
sleep_m0 <- brm(Reaction ~ Days, data = lme4::sleepstudy)
```

---
# Model summary

```{r }
summary(sleep_m0)
```

---
# View "fixed" effect
Let's look at our estimated relation between `Days` and `Reaction`

--
```{r }
conditional_effects(sleep_m0)
```

---
# Wrong model
Of course, this is the wrong model, we have a multilevel structure

```{r echo = FALSE, message = FALSE}
ggplot(lme4::sleepstudy, aes(Days, Reaction)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~Subject) +
  theme_minimal(15)
```

---
# Multilevel model
Notice the syntax is essentially equivalent to **lme4**

```{r eval = FALSE}
sleep_m1 <- brm(Reaction ~ Days + (Days | Subject), data = lme4::sleepstudy)
summary(sleep_m1)
```

```{r include = FALSE}
sleep_m1 <- brm(Reaction ~ Days + (Days | Subject), data = lme4::sleepstudy)
```

```{r echo = FALSE}
summary(sleep_m1)
```

---
# Fixed effect
The uncertainty has increased

```{r }
conditional_effects(sleep_m1)
```

---
# Checking your model

```{r }
pp_check(sleep_m1)
```

---
# More checks

```{r fig.height = 7}
plot(sleep_m1)
```

---
# Even more

```{r eval = FALSE}
launch_shinystan(sleep_m1)
```

---
# Model comparison

.footnote[See [here](https://arxiv.org/abs/1507.04544) for probably the best paper on this topic to date (imo)]

* Two primary (modern) methods

  + Leave-one-out Cross-Validation (LOO)
  
  + Widely Applicable Information Criterion (WAIC)


--
Both provide estimates of the *out-of-sample* predictive accuracy of the model. LOO is similar to K-fold CV, while WAIC is similar to AIC/BIC (but an improved version for Bayes models)


--
Both can be computed using **brms**. LOO is approximated (not actually refit each time). 


---
# LOO: M0

```{r }
loo(sleep_m0)
```

---
# LOO: M1

```{r }
loo(sleep_m1)
```

---
# Plot

```{r }
plot(loo(sleep_m1), label_points = TRUE)
```

---
# Interpretation

Large values here (anything above 0.7) is an indication of model misspecification

Smaller values do not guarantee a well-specified model, however


---
# LOO Compare

.footnote[This is a fairly complicated topic, and we won't spend a lot of time on it. See [here](https://mc-stan.org/loo/articles/loo2-example.html) for a bit more information specifically on the functions]

The best fitting model will be on top. Use the standard error within your interpretation

```{r }
loo_compare(loo(sleep_m0), loo(sleep_m1))
```


---
# WAIC
Similar to other information criteria.

```{r }
waic(sleep_m0)
waic(sleep_m1)
```

---
# Compare

You can use `waic` within `loo_compare()`

```{r }
loo_compare(waic(sleep_m0), waic(sleep_m1))
```

---
# Another model

```{r eval = FALSE}
kidney_m0 <- brm(time ~ age + sex, data = kidney)
pp_check(kidney_m0, type = "ecdf_overlay")
```

```{r include = FALSE}
kidney_m0 <- brm(time ~ age + sex, data = kidney)
```

```{r echo = FALSE}
pp_check(kidney_m0, type = "ecdf_overlay")
```

---
# Fixing this
We need to change the assumptions of our model - specifically that the outcome is not normally distributed

--
### Plot the raw data

```{r fig.height = 5.5}
ggplot(kidney, aes(time)) +
  geom_histogram(alpha = 0.7)
```


---
# Maybe Poisson?

(I realize we haven't talked about these types of models yet)

```{r eval = FALSE}
kidney_m1 <- brm(time ~ age + sex, 
                 data = kidney, 
                 family = poisson())
```

```{r include = FALSE}
kidney_m1 <- brm(time ~ age + sex, data = kidney, family = poisson())
```

---
# Nope

```{r }
pp_check(kidney_m1, type = "ecdf_overlay")
```


---
# Gamma w/log link
```{r eval = FALSE}
kidney_m2 <- brm(time ~ age + sex, 
                 data = kidney, 
                 family = Gamma("log"))
pp_check(kidney_m2, type = "ecdf_overlay")
```

```{r include = FALSE}
kidney_m2 <- brm(time ~ age + sex, data = kidney, family = Gamma("log"))
```

```{r echo = FALSE, fig.height = 6}
pp_check(kidney_m2, type = "ecdf_overlay")
```


---
# Specifying priors
Let's sample from *only* our priors to see what kind of predictions we get.

Here, we're specifying that our beta coefficient prior is $\beta \sim N(0, 0.5)$ 
  
```{r eval = FALSE}
kidney_m3 <- brm(
  time ~ age + sex, 
  data = kidney,
  family = Gamma("log"),
  prior = prior(normal(0, 0.5), class = "b"),
  sample_prior = "only"
)
```

```{r include = FALSE}
kidney_m3 <- brm(
  time ~ age + sex, 
  data = kidney,
  family = Gamma("log"),
  prior = prior(normal(0, 0.5), class = "b"),
  sample_prior = "only"
)
```

---
```{r }
kidney_m3
```

---
# Prior predictions
Random sample of 100 points

```{r echo = FALSE}
kidney %>% 
  tidybayes::add_fitted_draws(kidney_m3) %>% 
  ungroup() %>% 
  sample_n(100) %>% 
  ggplot(aes(time, .value)) +
  geom_point() +
  facet_wrap(~sex) +
  scale_y_log10(labels = scales::comma)
```

---
# Why?
It seemed like our prior was fairly tight


--
The exploding prior happens because of the log transformation

* Age is coded in years

* Imagine a coef of 1 (2 standard deviations above our prior)

* Prediction for a 25 year old would be $\exp(25)$ = `r exp(25)`

---
# A note on prior specifications
* It's hard

* I don't have a ton of good advice

* Be particularly careful when you're using distributions that have anything other than an identity link (e.g., log link, as we are here)

---
# One more model
Let's fit a model we've fit previously


--
In Week 4, we fit this model

```{r }
library(lme4)
popular <- read_csv(here::here("data", "popularity.csv"))
m_lmer <- lmer(popular ~ extrav + (extrav|class), popular,
               control = lmerControl(optimizer = "bobyqa"))
```

--
Try fitting the same model with **{brms}** with the default, diffuse priors

`r countdown::countdown(2)`

---
# Bayesian verision

```{r include = FALSE}
m_brms <- brm(popular ~ extrav + (extrav|class), popular)
m_brms
```

```{r eval = FALSE}
m_brms <- brm(popular ~ extrav + (extrav|class), popular)
```

```{r echo = FALSE}
m_brms
```


---
# lme4 model
```{r }
arm::display(m_lmer)
```

---
# Add priors

* Multiple ways to do this

* Generally, I allow the intercept to just be what it is

* For "fixed" effects, you might consider [regularizing](https://vasishth.github.io/bayescogsci/book/sec-sensitivity.html#regularizing-priors) priors.

* You can also set priors for individual parameters

* In my experience, you  will mostly want to set priors for  different "class"es of parameters, e.g.: `Intercept`, `b`, `sd`, `sigma`, `cor`

---
# Half-Cauchy distribution
This is the distribution most often used for standard deviation priors
```{r echo = FALSE}
library(extraDistr)
ggplot(data.frame(x = c(-1:10)), aes(x)) +
  stat_function(fun = ~dhcauchy(., 1),
                aes(color = "1")) +
  stat_function(fun = ~dhcauchy(., 2),
                aes(color = "2")) +
  stat_function(fun = ~dhcauchy(., 3),
                aes(color = "3"))  +
  scale_color_brewer("Sigma", palette = "Accent") +
  theme(legend.position = "bottom")
```

---
# Specify some new priors
Let's specify some regularizing priors for the fixed effects and standard deviations

--
```{r include = FALSE}
priors <- c(
  prior(normal(0, 0.5), class = b),
  prior(cauchy(0, 1), class = sd)
)

m_brms2 <- brm(popular ~ extrav + (extrav|class), 
               data = popular,
               prior = priors)
```

```{r eval = FALSE}
priors <- c(
  prior(normal(0, 0.5), class = b),
  prior(cauchy(0, 1), class = sd)
)

m_brms2 <- brm(popular ~ extrav + (extrav|class), 
               data = popular,
               prior = priors)
```

---
Almost no difference in this case

```{r }
m_brms2
```

---
# Speed
We'll never reach the `lme4::lmer()` speeds, but we can make it faster.

* Parallelize (not always as effective as you might hope)

* Use the cmdstanr backend
  + This requires a little bit of additional work, but is probably worth it if you're
  fitting bigger models.

```{r eval = FALSE}
install.packages(
  "cmdstanr",
  repos = c(
    "https://mc-stan.org/r-packages/", 
    getOption("repos")
  )
)
```

---
# Timings
I'm not evaluating the below, but the timings were 114.368 seconds and 87.383 seconds, respectively.

```{r eval = FALSE}
library(tictoc)

tic()
m_brms <- brm(popular ~ extrav + (extrav|class), 
              data = popular)
toc()

tic()
m_brms2 <- brm(popular ~ extrav + (extrav|class), 
               data = popular,
               backend = "cmdstanr") #<<
toc()
```

---
class: inverse-red middle
# Wrapping up our Bayes intro


---
# Advantages to Bayes
* Opportunity to incorporate prior knowledge into the modeling process (you don't really *have* to - could just set wide priors)

* Natural interpretation of uncertainty

* Can often allow you to estimate models that are difficult if not impossible with frequentist methods


--

## Disadvatages

* Generally going to be slower in implementation

* You may run into pushback from others - particularly with respect to priors

---
# Notes on the posterior
* The posterior is the distribution of the parameters, given the data

* Think of it as the distribution of what we don't know, but are interested in (model parameters), given what we know or have observed (the data), and our prior beliefs

* Gives a complete picture of parameter uncertainty

* We can do lots of things with the posterior that is hard to get otherwise

---
class: inverse-green middle
# Break

.center[
`r countdown::countdown(5)`
]

---
class: inverse-red middle
# Review of logistic regression

---
# Data generating distribution
Up to this point, we've been assuming the data were generated from a normal distribution.

--

For example, we might fit a simple linear regression model to the wages data like this

$$
\begin{aligned}
  \operatorname{wages}_{i}  &\sim N \left(\widehat{y}, \sigma^2 \right) \\
  \widehat{y} &= \alpha + \beta_{1}(\operatorname{exper})
\end{aligned}
$$

where we're embedding a linear model into the mean

--
In code

```{r }
wages <- read_csv(here::here("data", "wages.csv")) %>% 
  mutate(hourly_wage = exp(lnw))

wages_lm <- lm(hourly_wage ~ exper, data = wages)
```

---
# Graphically

```{r}
ggplot(wages, aes(exper, hourly_wage)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

--
Aside - you see why the log of wages was modeled instead?

---
# Log wages

```{r }
ggplot(wages, aes(exper, lnw)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

---
# Move to binary model
Let's split the wages data into a binary classification based on whether it's above or below the mean.

--

```{r }
wages <- wages %>% 
  mutate(
    high_wage = ifelse(
      hourly_wage > mean(hourly_wage, na.rm = TRUE), 1, 0
    )
  )

wages %>% 
  select(id, hourly_wage, high_wage)
```

---
# Plot

```{r fig.height = 5}
means <- wages %>% 
  group_by(high_wage) %>% 
  summarize(mean = mean(exper))

ggplot(wages, aes(exper, high_wage)) +
  geom_point(alpha = 0.01) +
  geom_point(aes(x = mean), data = means, 
             shape = 23, 
             fill = "cornflowerblue")
```

---
# We could fit a linear model

```{r }
m_lpm <- lm(high_wage ~ exper, data = wages)
arm::display(m_lpm)
```

--
This is referred to as a linear probability model (LPM) and they are pretty hotly contested, with proponents and detractors

---
# Plot

```{r }
ggplot(wages, aes(exper, high_wage)) +
  geom_point(alpha = 0.01) +
  geom_smooth(method = "lm", se = FALSE)
```

---
# Prediction

What if somebody has an experience of 25 years?

--

```{r }
predict(m_lpm, newdata = data.frame(exper = 25))
```

🤨

Our prediction goes outside the range of our data.


--
As a rule, the assumed data generating process should match the boundaries of the data.


--
Of course, you could truncate after the fact. I think that's less than ideal (see [here](https://www.alexpghayes.com/blog/consistency-and-the-linear-probability-model/) or [here](https://github.com/alexpghayes/linear-probability-model/blob/master/presentation.pdf) for discussions contrasting LPM with logistic regression).

---
# The binomial model

$$
y_i \sim \text{Binomial}(n, p_i)
$$

--
Think in terms of coin flips


--
$n$ is the number of coin flips, while $p_i$ is the probability of heads


---
# Example
Flip 1 coin 10 times

```{r }
set.seed(42)
rbinom(
  n = 10, # number of trials
  size = 1, # number of coins
  prob = 0.5 # probability of heads
)
```

--
Side note - a binomial model with `size = 1` (or $n = 1$ in equation form) is equivalent to a Bernoulli distribution


--

Flip 10 coins 1 time

```{r }
rbinom(n = 1, size = 10, prob = 0.5)
```

---
# Modeling
We now build a linear model for $p$, just like we previously built a linear model for $\mu$.


--
.center[
.realbig[
A problem
]
]

Probability is bounded $[0, 1]$ 

We need to ensure that our model respects these bounds


---
# Link functions

We solve this problem by using a *link* function

$$
\begin{aligned}
y_i &\sim \text{Binomial}(n, p_i) \\
f(p_i) &= \alpha + \beta(x_i) 
\end{aligned}
$$

--
* Instead of modeling $p_i$ directly, we model $f(p_i)$


--
* The specific $f$ is the link function


--
* Link functions map the *linear* space of the model to the *non-linear* parameters (like probability)


--
* The *log* and *logit* links are most common


---
# Binomial logistic regression
If we only have two categories, we commonly assume a binomial distribution, with a logit link.

$$
\begin{aligned}
y_i &\sim \text{Binomial}(n, p_i) \\
\text{logit}(p_i) &= \alpha + \beta(x_i) 
\end{aligned}
$$

--
where the logit link is defined by the *log-odds*

$$
\text{logit}(p_i) = \log\left[\frac{p_i}{1-p_i}\right]
$$


--
So

$$
\log\left[\frac{p_i}{1-p_i}\right] = \alpha + \beta(x_i) 
$$

---
# Inverse link

What we probably want to interpret is probability

--

We can transform the log-odds to probability by exponentiating 

$$
p_i = \frac{\exp(\alpha + \beta(x_i))}{1 + \exp(\alpha + \beta(x_i))}
$$
--
This is the logistic function, or the inverse-logit.


---
# Example logistic regression model

```{r}
m_glm <- glm(high_wage ~ exper, 
             data = wages, 
             family = binomial(link = "logit")) #<<
arm::display(m_glm)
```


---
# Coefficient interpretation
The coefficients are reported on the *log-odds* scale. Other than that, interpretation is the same.


--
For example:

The log-odds of a participant with zero years experience being in the high wage category was `r round(coef(m_glm)[1], 2)`. 

For every one year of additional experience, the log-odds of being in the high wage category increased by `r round(coef(m_glm)[2], 2)`.


---
# Note
* Outside of scientific audiences, almost nobody is going to understand the previous slide

* You *cannot* just transform the coefficients and interpret them as probabilities (because it is non-linear on the probability scale).


---
# Log odds scale

```{r fig.height = 5}
tibble(exper = 0:25) %>% 
  mutate(pred = predict(m_glm, newdata = .)) %>% 
  ggplot(aes(exper, pred)) +
  geom_line()
```

--
Perfectly straight line - change in log-odds are modeled as a linear function of experience

---
# Probability scale

```{r fig.height = 5}
tibble(exper = 0:25) %>% 
  mutate(pred = predict(m_glm, 
                        newdata = ., 
                        type = "response")) %>% 
  ggplot(aes(exper, pred)) +
  geom_line()
```

--
Our model parameters map to probability non-linearly, and it is bound to $[0, 1]$

---
# Probability predictions

Let's make the predictions from the previous slide "by hand"

--
Recall:

$$
p_i = \frac{\exp(\alpha + \beta(x_i))}{1 + \exp(\alpha + \beta(x_i))}
$$

--
And our coefficients are

```{r }
coef(m_glm)
```

---
# Intercept

$$
(p_i|\text{exper = 0}) = \frac{\exp(-1.65 + 0.25(0))}{1 + \exp(-1.65 + 0.25(0))}
$$

--
$$
(p_i|\text{exper = 0}) = \frac{\exp(-1.65)}{1 + \exp(-1.65)}
$$

--
$$
(p_i|\text{exper = 0}) = \frac{0.19}{1.19} = 0.16
$$

---
# Five years experience

Notice the exponentiation happens *after* adding the coefficients together

$$
(p_i|\text{exper = 5}) = \frac{\exp(-1.65 + 0.25(5))}{1 + \exp(-1.65 + 0.25(5))}
$$

--
$$
(p_i|\text{exper = 5}) = \frac{\exp(-0.4)}{1 + \exp(-0.4)}
$$

--
$$
(p_i|\text{exper = 5}) = \frac{0.67}{1.67} = 0.40
$$

---
# Fifteen years experience

$$
(p_i|\text{exper = 15}) = \frac{\exp(-1.65 + 0.25(15))}{1 + \exp(-1.65 + 0.25(15))}
$$

--
$$
(p_i|\text{exper = 15}) = \frac{\exp(2.1)}{1 + \exp(2.1)}
$$

--
$$
(p_i|\text{exper = 15}) = \frac{8.16}{9.16} = 0.89
$$


---
# More interpretation

Let's try to make this so more people might understand.


--
First, show the logistic curve on the probability scale! Can help prevent linear interpretations


--
Discuss probabilities at different $x$ values


--
The "divide by 4" rule

---
# Divide by 4

* Logistic curve is steepest when $p_i = 0.5$

* The slope of the logistic curve (its derivative) is maximized at $\beta/4$

* Aside from the intercept, we can say that the change is *no more* than $\beta/4$ 

--
## Example

$$
\frac{0.25}{4} = 0.0625
$$

--
So, a one year increase in experience corresponds to, at most, a 6% increase in being in the high wage category


---
# Example writeup

There was a `r round(predict(m_glm, newdata = tibble(exper = 0), type = "response") * 100)` percent chance that a participant with zero years experience would be in the *high wage* category. The logistic function, which mapped years of experience to the  probability of being in the *high-wage* category, was non-linear, as shown in Figure 1. At its steepest point, a one year increase in experience corresponded with approximately a 6% increase in the probability of being in the high-wage category. For individuals with 5, 10, and 15 years of experience, the probability increased to a `r round(predict(m_glm, newdata = tibble(exper = 5), type = "response") * 100)`, `r round(predict(m_glm, newdata = tibble(exper = 10), type = "response") * 100)`, and `r round(predict(m_glm, newdata = tibble(exper = 15), type = "response") * 100)` percent chance, respectively. 

---
class: inverse-red middle
# Other distributions

---
background-image: url(img/distributions.png)
background-size: contain

.footnote[Figure from [McElreath](https://xcelab.net/rm/statistical-rethinking/)]

---
class: inverse-blue middle

# Multilevel logistic regression

---
# The data
Polling data from the 1988 election.

```{r }
polls <- rio::import(here::here("data", "polls.dta"),
                     setclass = "tbl_df")
polls
```

---
# About the data

* Collected one week before the election

* Nationally representative sample

* Should use post-stratification to control for non-response, but we'll hold off on that for now (See Gelman & Hill, Chapter 14 for more details)

---
# Baseline probability

Let's assume we want to estimate the probability that Bush will be elected.


--
We could fit a single level model like this

$$
\begin{aligned}
\operatorname{bush} &\sim \text{Binomial}\left(n = 1, \operatorname{prob}_{\operatorname{bush} = \operatorname{1}}= \hat{P}\right) \\
 \log\left[ \frac { \hat{P} }{ 1 - \hat{P} } \right] 
 &= \alpha
\end{aligned}
$$

---
# In code

Can you write the code for the previous model?

`r countdown::countdown(1)`

--

```{r}
bush_sl <- glm(bush ~ 1, 
               data = polls,
               family = binomial(link = "logit"))
```


--

```{r }
arm::display(bush_sl)
```

--
So, $p_i = \frac{\exp(0.25)}{1 + \exp(0.25)} = 0.56$

---
# State-level variability

To estimate state-level variability, we just specify a distribution for the intercept variability.

--

$$
\begin{aligned}
\operatorname{bush} &\sim \text{Binomial}\left(n = 1, \operatorname{prob}_{\operatorname{bush} = \operatorname{1}}= \hat{P}\right) \\
 \log\left[ \frac { \hat{P} }{ 1 - \hat{P} } \right] 
 &= \alpha_{j[i]} \\
    \alpha_{j}  &\sim N \left(\mu_{\alpha_{j}}, \sigma^2_{\alpha_{j}} \right)
    \text{, for state j = 1,} \dots \text{,J}
\end{aligned}
$$

--
Notice we're still specifying that the intercept variability is generated by a *normal* distribution.


--
What does this variability actually represent?


--
Variance in the log-odds

---
# Fitting the model

If we're using **{lme4}**, we just swap `lmer()` for `glmer()` and specify the family and link function.

--

```{r }
library(lme4)

m0 <- glmer(bush ~ 1 + (1|state),
           data = polls,
           family = binomial(link = "logit"))
arm::display(m0)
```

---
# Interpretation

* The average log odds of supporting bush was 0.25

* This average varied between states with a standard deviation of 0.34

---
# State-level variation

```{r }
library(broom.mixed)
m0_tidied <- tidy(m0, effects = "ran_vals", conf.int = TRUE)
m0_tidied
```

---
# Fancified Plot Code
```{r eval = FALSE}
m0_tidied %>% 
  mutate(level = forcats::fct_reorder(level, estimate)) %>% 
  ggplot(aes(estimate, level)) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  geom_point(aes(color = estimate)) +
  geom_vline(xintercept = 0, color = "gray40", size = 3) +
  labs(x = "Log-Odds Estimate", y = "") +
  colorspace::scale_color_continuous_diverging(palette = "Blue-Red 2") +
  guides(color = "none") +
  theme(panel.grid.major.y = element_blank(),
        axis.text.y = element_blank())
```

---
# Fancified Plot
```{r echo = FALSE, fig.height = 9}
m0_tidied %>% 
  mutate(level = forcats::fct_reorder(level, estimate)) %>% 
  ggplot(aes(estimate, level)) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  geom_point(aes(color = estimate)) +
  geom_vline(xintercept = 0, color = "gray40", size = 3) +
  labs(x = "Log-Odds Estimate", y = "") +
  colorspace::scale_color_continuous_diverging(palette = "Blue-Red 2") +
  guides(color = "none") +
  theme(panel.grid.major.y = element_blank(),
        axis.text.y = element_blank())
```

---
# Extending the model

Let's add some predictors

* Include `age`, `female` and `black` as fixed effect predictors. 


--
* We should center age so the intercept represents the sample average age.


--
You try first

`r countdown::countdown(3)`

---
```{r }
polls <- polls %>% 
  mutate(age_c = age - mean(age, na.rm = TRUE))

m1 <- glmer(bush ~ age_c + female + black + (1|state),
            data = polls,
            family = binomial(link = "logit"))
arm::display(m1)
```

---
# Varying slopes
The average probability of a respondent who was coded `black == 1` who was the average age and non-female supporting Bush was

$$
\small
\begin{equation}
(p_i|\text{age} = 0, \text{female} = 0, \text{black} = 1) = \frac{\exp(0.43 + (-0.08 \times 0) + (-0.11 \times 0) + (-1.84 \times 1))}{1 + \exp(0.43 + (-0.08 \times 0) + (-0.11 \times 0) + (-1.84 \times 1))}
\end{equation}
$$

--
$$
(p_i|\text{age} = 0, \text{female} = 0, \text{black} = 1) = \frac{\exp(0.43 -1.84))}{1 + \exp(0.43  -1.84)}
$$

--
$$
(p_i|\text{age} = 0, \text{female} = 0, \text{black} = 1) = \frac{\exp(-1.41))}{1 + \exp(-1.41)}
$$

--
$$
(p_i|\text{age} = 0, \text{female} = 0, \text{black} = 1) = \frac{0.24}{1.24}
$$

--
$$
(p_i|\text{age} = 0, \text{female} = 0, \text{black} = 1) = 0.19
$$

---
# Vary by state?
You try first - try to fit a model that estimates between-state variability in the relation between individuals coded `black`, and their probability of voting for Bush.

`r countdown::countdown(1)`

--

```{r }
m2 <- glmer(bush ~ age_c + female + black + (black|state),
            data = polls,
            family = binomial(link = "logit"))
arm::display(m2)
```

---
# Look at random effects

```{r }
ranef_m2 <- tidy(m2, effects = "ran_vals", conf.int = TRUE) %>% 
  arrange(level)
ranef_m2
```

---
# Include fixed effects

```{r }
fe <- data.frame(
  fixed = fixef(m2),
  term = names(fixef(m2))
)
fe

ranef_m2 <- left_join(ranef_m2, fe)
ranef_m2 %>% 
  select(level, term, estimate, fixed)
```

---
# Add in fixed effects
```{r }
ranef_m2 <- ranef_m2 %>% 
  mutate(estimate = estimate + fixed)
ranef_m2
```

---
# Compute log-odds
Compute means for each group - i.e., add the coefficients

```{r}
to_plot <- ranef_m2 %>% 
  group_by(level) %>% 
  mutate(estimate = cumsum(estimate)) %>% 
  ungroup()

to_plot
```

---
# Create factor level

```{r }
lev_order <- to_plot %>% 
  filter(term == "(Intercept)") %>% 
  mutate(lev = forcats::fct_reorder(level, estimate))

to_plot <- to_plot %>% 
  mutate(level = factor(level, levels = levels(lev_order$lev)))
```

---
# Plot
```{r eval = FALSE}
# data transform
to_plot %>% 
  mutate(group = ifelse(term == "(Intercept)", "Non-Black", "Black")) %>%
  
  # plot
  ggplot(aes(prob, level)) +
  geom_line(aes(group = level), color = "gray60", size = 1.2) +
  geom_point(aes(color = group)) +
  geom_vline(xintercept = 0.5, color = "gray40", size = 3) +
  
  # themeing stuff
  labs(x = "Probability Estimate", y = "") +
  xlim(0, 1) +
  scale_color_brewer(palette = "Set2") +
  theme(panel.grid.major.y = element_blank(),
        axis.text.y = element_blank())
```

---
# Plot
```{r echo = FALSE, fig.height = 9}
to_plot %>% 
  mutate(group = ifelse(term == "(Intercept)", "Non-Black", "Black")) %>%
  ggplot(aes(estimate, level)) +
  geom_line(aes(group = level), color = "gray60", size = 1.2) +
  geom_point(aes(color = group)) +
  geom_vline(xintercept = 0, color = "gray40", size = 3) +
  labs(x = "Log-Odds Estimate", y = "") +
  scale_color_brewer(palette = "Set2") +
  theme(panel.grid.major.y = element_blank(),
        axis.text.y = element_blank())
```


---
# Probability scale

```{r eval = FALSE, fig.height = 9}
# data transform
to_plot %>% 
  mutate(
    group = ifelse(term == "(Intercept)", "Non-Black", "Black"),
    prob = exp(estimate)/(1 + exp(estimate)) #<<
  ) %>%
  
  # plot
  ggplot(aes(prob, level)) +
  geom_line(aes(group = level), color = "gray60", size = 1.2) +
  geom_point(aes(color = group)) +
  geom_vline(xintercept = 0.5, color = "gray40", size = 3) +
  
  # themeing stuff
  labs(x = "Probability Estimate", y = "") +
  xlim(0, 1) +
  scale_color_brewer(palette = "Set2") +
  theme(panel.grid.major.y = element_blank(),
        axis.text.y = element_blank())
```

---
# Probability scale

```{r echo = FALSE, fig.height = 9}
to_plot %>% 
  mutate(
    group = ifelse(term == "(Intercept)", "Non-Black", "Black"),
    prob = exp(estimate)/(1 + exp(estimate)) #<<
  ) %>%
  ggplot(aes(prob, level)) +
  geom_line(aes(group = level), color = "gray60", size = 1.2) +
  geom_point(aes(color = group)) +
  geom_vline(xintercept = 0.5, color = "gray40", size = 3) +
  labs(x = "Probability Estimate", y = "") +
  xlim(0, 1) +
  scale_color_brewer(palette = "Set2") +
  theme(panel.grid.major.y = element_blank(),
        axis.text.y = element_blank())
```


---
class: inverse-blue middle

# Bayes

---
# Refit
Can you fit the same model we just fit, but with **{brms}**?

You try first

Feel free to just use flat priors

`r countdown::countdown(2)`

---
# Flat priors

```{r }
library(brms)
m2_brms <- brm(bush ~ age + female + black + (black|state),
            data = polls,
            family = bernoulli(link = "logit"),
            backend = "cmdstan",
            cores = 4)
```

---
# Model summary

```{r }
summary(m2_brms)
```

---
# Posterior predictive check

```{r }
pp_check(m2_brms, type = "bars")
```

---
# Posterior predictive check 2

```{r }
pp_check(m2_brms, type = "stat")
```

---
class: inverse-green middle
# Next time
* More Bayes for binomial logistic regression
* Plotting Bayes models
* Missing data 
* Piece-wise growth models


