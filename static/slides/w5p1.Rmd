---
title: "Variance-Covariance Matrices"
author: "Daniel Anderson "
date: "Week 4"
output:
  xaringan::moon_reader:
    css: ["default", "new.css"]
    lib_dir: libs
    nature:
      navigation:
        scroll: false
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "https://platform.twitter.com/widgets.js"
    includes:
      in_header: "load-feather.html"
---

```{r include = FALSE, results = "asis"}
source(here::here("static", "slides", "slide-setup.R"))
xaringanExtra::use_clipboard()
library(tidyverse)
library(equatiomatic)
theme_set(theme_minimal(25))
knitr::opts_chunk$set(fig.width = 13, message = FALSE)
update_geom_defaults('errorbarh', list(size = 2))
update_geom_defaults('path', list(size = 3))
update_geom_defaults('point', list(size = 5))
update_geom_defaults('text', list(size = 9))
```

`r setup("w5p1")`

---
# Agenda

* Review some notation

* Unstructured VCV Matrices and alternatives


---
# Learning Objectives

* Gain a deeper understanding of how the residual structure is different in multilevel models

* Understand that there are methods for changing the residual structure, and understand when and why this might be preferable

* Be able to implement alternative methods using [**{nlme}**](https://cran.r-project.org/web/packages/nlme/)


---
class: inverse-blue middle

# Notation
### Reviewing some Gelman and Hill notation


---
# Data
Let's start by using the *sleepstudy* data from **{lme4}**.

```{r }
library(lme4)
head(sleepstudy)
```

---
# Translate
Translate the following model into `lme4::lmer()` syntax

```{r echo = FALSE}
ss0 <- lmer(Reaction ~ 1 + (1|Subject), data = sleepstudy)
extract_eq(ss0)
```

`r countdown::countdown(2)`

--
```{r eval = FALSE}
lmer(Reaction ~ 1 + (1|Subject), data = sleepstudy)
```

---
# More complicated

Translate this equation into `lme4::lmer()` syntax

```{r echo = FALSE}
ss1 <- lmer(Reaction ~ Days + (1|Subject), data = sleepstudy)
extract_eq(ss1)
```


`r countdown::countdown(1)`

--
```{r eval = FALSE}
lmer(Reaction ~ Days + (1|Subject), data = sleepstudy)
```


---
# Even more complicated

Translate this equation into `lme4::lmer()` syntax

```{r echo = FALSE}
ss2 <- lmer(Reaction ~ Days + (Days|Subject), data = sleepstudy)
extract_eq(ss2)
```


`r countdown::countdown(1)`

--
```{r eval = FALSE}
lmer(Reaction ~ Days + (Days|Subject), data = sleepstudy)
```


---
# New data

```{r}
sim3 <- read_csv(here::here("data", "sim3level.csv"))
sim3
```

`r countdown::countdown(1)`

.footnote[Data obtained [here](http://alexanderdemos.org/Mixed5.html)]

---
# A bit of a problem
```{r }
sim3 %>% 
  count(Classroom, School)
```

---
# Make classroom unique
We could handle this with our model syntax, but nested IDs like this always makes me nervous. Let's make them unique.

--
```{r }
sim3 <- sim3 %>% 
  mutate(class_id = paste0("class", Classroom, ":", School))
sim3
```

---
# New model

`r countdown::countdown(2)`

Translate this equation into code

```{r echo = FALSE}
s0 <- lmer(Math ~ ActiveTime + (ActiveTime|class_id) + (1|School),
           data = sim3)
extract_eq(s0)
```


--
```{r eval = FALSE}
lmer(Math ~ ActiveTime + (ActiveTime|class_id) + (1|School),
     data = sim3)
```

---
# This one
What about this one?

`r countdown::countdown(2)`

```{r echo = FALSE}
s1 <- lmer(Math ~ ActiveTime + ClassSize + (ActiveTime|class_id) + (
  ActiveTime|School),
           data = sim3)
extract_eq(s1, font_size = "small")
```

--
```{r eval = FALSE}
lmer(Math ~ ActiveTime + ClassSize + 
       (ActiveTime|class_id) + (ActiveTime|School),
     data = sim3)
```

---
# Last one

What about this one?

`r countdown::countdown(2)`

```{r echo = FALSE}
s2 <- lmer(Math ~ ActiveTime * ClassSize + (ActiveTime|class_id) + (
  ActiveTime + ClassSize|School),
           data = sim3)
extract_eq(s2, font_size = "small")
```

--
```{r eval = FALSE}
lmer(Math ~ ActiveTime * ClassSize + 
       (ActiveTime|class_id) + (ActiveTime + ClassSize|School),
     data = sim3)
```


---
class: inverse-blue middle

# Residual structures

---
# Data
[Willett, 1988](https://www.jstor.org/stable/1167368?seq=1#metadata_info_tab_contents)

* $n$ = 35 people
* Each completed a cognitive inventory on "opposites naming"
* At first time point, participants also completed a general cognitive measure

---
# Read in data

```{r message = FALSE}
willett <- read_csv(here::here("data", "willett-1988.csv"))
willett
```

---
# Standard OLS 

* We have four observations per participant. 

* If we fit a standard OLS model, it would look like this

```{r }
bad <- lm(opp ~ time, data = willett)
summary(bad)
```

---
# Assumptions
As we discussed previously, this model looks like this

$$
\operatorname{opp} = \alpha + \beta_{1}(\operatorname{time}) + \epsilon
$$

where

$$
\epsilon \sim \left(0, \sigma \right)
$$

---
# Individual level residuals
We can expand our notation, so it looks like a multivariate normal distribution

$$
\begin{equation}
  \left(
    \begin{array}{c}
      \epsilon_1 \\\
      \epsilon_2 \\\
      \epsilon_3 \\\
      \vdots \\\
      \epsilon_n
    \end{array}
  \right)
\sim MVN
  \left(
    \left[
      \begin{array}{c}
        0\\\
        0\\\
        0\\\
        \vdots \\\
        0
    \end{array}
    \right],
    \left[
      \begin{array}{ccccc}
        \sigma_{\epsilon} & 0 & 0 & \dots & 0 \\\
        0 & \sigma_{\epsilon} & 0 & 0 & 0 \\\
        0 & 0 & \sigma_{\epsilon} & 0 & 0 \\\
        \vdots & 0 & 0 & \ddots & \vdots \\\
        0 & 0 & 0 & \dots & \sigma_{\epsilon}
      \end{array}
    \right]
  \right)
\end{equation}
$$

This is where the $i.i.d.$ part comes in. The residuals are assumed $i$ndependent and $i$dentically $d$istributed.

---
# Multilevel model
Very regularly, there are reasons to believe the $i.i.d.$ assumption is violated. Consider our current case, with 4 time points for each individual.

* Is an observation for one time point for one individual *independent* from the other observations for that individual?


--
* Rather than estimating a single residual variance, we estimate an additional components associated with individuals, leading to a *block* diagonal structure

---
# Block diagonal

.small70[
$$
\begin{equation}
  \left(
    \begin{array}{c}
      \epsilon_{11} \\
      \epsilon_{12} \\
      \epsilon_{13} \\
      \epsilon_{14} \\
      \epsilon_{21} \\
      \epsilon_{22} \\
      \epsilon_{23} \\
      \epsilon_{24} \\
      \vdots \\
      \epsilon_{n1} \\
      \epsilon_{n2} \\
      \epsilon_{n3} \\
      \epsilon_{n4}
    \end{array}
  \right)
\sim
  \left(
    \left[
      \begin{array}{c}
        \\
        0\\
        0\\
        0\\
        0\\
        0\\
        0\\
        0\\
        0\\
        \vdots \\
        0\\
        0\\
        0\\
        0
      \end{array}
    \right],
    \left[
      \begin{array}{{@{}*{13}c@{}}}
      \\
        \sigma_{11} & \sigma_{12} & \sigma_{13} & \sigma_{14} & 0 & 0 & 0 & 0 & \dots  & 0 & 0 & 0 & 0 \\
        \sigma_{21} & \sigma_{22} & \sigma_{23} & \sigma_{24} & 0 & 0 & 0 & 0 & \dots  & 0 & 0 & 0 & 0 \\
        \sigma_{31} & \sigma_{32} & \sigma_{33} & \sigma_{34} & 0 & 0 & 0 & 0 & \dots  & 0 & 0 & 0 & 0 \\
        \sigma_{41} & \sigma_{42} & \sigma_{43} & \sigma_{44} & 0 & 0 & 0 & 0 & \dots  & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & \sigma_{11} & \sigma_{12} & \sigma_{13} & \sigma_{14} & \dots  & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & \sigma_{21} & \sigma_{22} & \sigma_{23} & \sigma_{24} & \dots  & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & \sigma_{31} & \sigma_{32} & \sigma_{33} & \sigma_{34} & \dots  & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & \sigma_{41} & \sigma_{42} & \sigma_{43} & \sigma_{44} & \dots  & 0 & 0 & 0 & 0 \\
        \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots \\
        0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \dots & \sigma_{11} & \sigma_{12} & \sigma_{13} & \sigma_{14} \\
        0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \dots & \sigma_{21} & \sigma_{22} & \sigma_{23} & \sigma_{24} \\
        0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \dots & \sigma_{31} & \sigma_{32} & \sigma_{33} & \sigma_{34} \\
        0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \dots & \sigma_{41} & \sigma_{42} & \sigma_{43} & \sigma_{44}
      \end{array}
    \right]
  \right)
\end{equation}
$$
]


--
Correlations for  off-diagonals estimated


--
Same variance components for all blocks


--
Out-of-block diagonals are still zero

---
# Homogeneity of variance
As mentioned on the previous slide, we assume the same variance components across all student

This is referred to as the homogeneity of variance assumption - although the block (often referred to as the composite residual) may be heteroscedastic and dependent **within** a grouping factor (i.e., people) the entire error structure is repeated identically across units (i.e., people)

---
# Block diagonal
Because of the homogeneity of variance assumption, we can re-express our block diagonal design as follows

$$
\begin{equation}
r \sim N \left(\boldsymbol{0}, 
  \left[
    \begin{array}{ccccc}
      \boldsymbol{\Sigma_r} & \boldsymbol{0} & \boldsymbol{0} & \dots & \boldsymbol{0} \\
      \boldsymbol{0} & \boldsymbol{\Sigma_r} & \boldsymbol{0} & \dots & \boldsymbol{0} \\
      \boldsymbol{0} & \boldsymbol{0} & \boldsymbol{\Sigma_r} &  \dots & \boldsymbol{0} \\
      \vdots & \vdots & \vdots & \ddots & \vdots & \\
      \boldsymbol{0} & \boldsymbol{0} &  \boldsymbol{0} & \dots & \boldsymbol{\Sigma_r}
    \end{array}
  \right]
\right)
\end{equation}
$$
---
# Composite residual
We then define the composite residual, which is common across units

$$
\begin{equation}
  \boldsymbol{\Sigma_r} = \left[
    \begin{array}{cccc}
      \sigma_{11} & \sigma_{12} & \sigma_{13} & \sigma_{14} \\
      \sigma_{21} & \sigma_{22} & \sigma_{23} & \sigma_{24} \\
      \sigma_{31} & \sigma_{32} & \sigma_{33} & \sigma_{34} \\
      \sigma_{41} & \sigma_{42} & \sigma_{43} & \sigma_{44}
    \end{array}
  \right]
\end{equation}
$$

---
# Let's try!

Let's fit a parallel slopes model with the Willett data. You try first.

`r countdown::countdown(1)`

--
```{r }
w0 <- lmer(opp ~ time + (1|id), willett)
```

--
What does the residual variance-covariance look like? Let's use **sundry** to pull it 

```{r }
library(sundry)
w0_rvcv <- pull_residual_vcov(w0)
```

---
# Image
Sparse matrix - we can view it with `image()`

```{r }
image(w0_rvcv)
```

---
# Pull first few rows/cols

```{r }
w0_rvcv[1:8, 1:8]
```

---
# Structure
On the previous slide, note the values on the diagonal are all the same, as are all the off-diagonals

* This is because we've only estimated one additional variance component


---
# Understanding these numbers
Let's look at the model output

```{r }
arm::display(w0)
```

---
The diagonal values were $`r w0_rvcv[1, 1]`$ while the off diagonal values were $`r w0_rvcv[1, 2]`$

Let's extract the variance components from our model.

```{r }
vars_w0 <- as.data.frame(VarCorr(w0))
vars_w0
```

--

Notice anything?


--

The diagonals are given by `sum(vars_w0)$vcov` while the off-diagonals are just the intercept variance


---
# Including more complexity

Try estimating this model now, then look at the residual variance-covariance matrix again

```{r echo = FALSE}
w1 <- lmer(opp ~ time + (time|id), willett)
extract_eq(w1)
```

`r countdown::countdown(4)`

---
# The composite residual
```{r }
w1 <- lmer(opp ~ time + (time|id), willett)
w1_rvcv <- pull_residual_vcov(w1)
w1_rvcv[1:4, 1:4]
```


--
### Unstructured

The model we fit has an *unstructured* variance co-variance matrix. While each
block is the same, every element of the block is now estimated.


---
# What are these numbers?
They are the variance components, re-expressed as a composite residual


--
The diagonal is given by

$$
\begin{equation}
\sigma^2 + \sigma^2_{\alpha_j} + 2\sigma^2_{01}w_i + \sigma^2_{\beta_1}w^2_i 
\end{equation}
$$


where $w$ represents the given wave (for our example)


--
Let's do this "by hand"

---
# Get the pieces

```{r }
vars_w1 <- as.data.frame(VarCorr(w1))

# get the pieces
int_var <- vars_w1$vcov[1]
slope_var <- vars_w1$vcov[2]
covar <- vars_w1$vcov[3]
residual <- vars_w1$vcov[4]
```

---
# Calculate

```{r }
diag(w1_rvcv[1:4, 1:4])
residual + int_var
residual + int_var + 2*covar + slope_var
residual + int_var + (2*covar)*2 + slope_var*2^2
residual + int_var + (2*covar)*3 + slope_var*3^2
```

---
# Why?
What was the point of doing that by hand?

![](https://media1.popsugar-assets.com/files/thumbor/2qfH3BIwMUci7X3COzTN2VCEPL0/fit-in/728xorig/filters:format_auto-!!-:strip_icc-!!-/2014/07/09/780/n/1922283/4c633a9489b97031_tumblr_inline_mn9lfioyVS1qz4rgp/i/He-Basically-Invented-Side-Eye-Move.gif)

I don't really care about the equations - the point is, they're just transformations of the variance components


---
# Off-diagonals

The off-diagonals are given by

$$
\begin{equation}
\sigma^2_{\alpha_j} + \sigma_{01}(t_i + t_i') + \sigma^2_{\beta_1}t_it_i'
\end{equation}
$$

---
# Calculate a few
```{r label, options}
w1_rvcv[1:4, 1:4]
int_var + covar*(1 + 0) + slope_var*1*0
int_var + covar*(2 + 1) + slope_var*2*1
int_var + covar*(3 + 2) + slope_var*3*2
int_var + covar*(2 + 0) + slope_var*2*0
```

---
class: inverse-red middle
# Positing other structures

---
# The possibilities
There are a number of alternative structures. We'll talk about a few here.


--
If you want to go deeper, I suggest Singer & Willett, Chapter 7


--
Code to fit models with each type of structure, using the same Willett data we're using today, is available [here](https://stats.idre.ucla.edu/r/examples/alda/r-applied-longitudinal-data-analysis-ch-7/)

---
# Structures we'll fit

* Unstructured (default with **lme4**, we've already seen this)

* Variance components (no off-diagonals)

* Autoregressive

* Heterogeneous autoregressive

* Toeplitz


--
Note this is just a sampling. Other structures are possible.


--
Outside of *unstructured* & *variance component only* models, we'll need to use the **nlme** package


--
We could also use Bayes, as we'll get to in a few weeks.

---
class: inverse-orange middle
# Variance component only

---
# The model
Generally, in a model like the Willett data, we would estimate intercept variance, and maybe slope variance.


--
We saw earlier how these combine to create the residual variance-covariance matrix


--
Alternatively, we can estimate separate variances at each time point


--

$$
\begin{equation}
  \boldsymbol{\Sigma_r} = \left[
    \begin{array}{cccc}
      \sigma^2_1 & 0 & 0 & 0 \\
      0 & \sigma^2_2 & 0 & 0 \\
      0 & 0 & \sigma^2_3 & 0 \\
      0 & 0 & 0 & \sigma^2_4
    \end{array}
  \right]
\end{equation}
$$

---
# One-hot encoding

First, use one-hot encoding for time

Same as dummy-coding, but you use *all* the levels.

```{r }
w <- willett %>% 
  mutate(t0 = ifelse(time == 0, 1, 0),
         t1 = ifelse(time == 1, 1, 0),
         t2 = ifelse(time == 2, 1, 0),
         t3 = ifelse(time == 3, 1, 0))
w
```

---
# Alternative creation
The `model.matrix()` function automatically dummy-codes factors.

--
```{r }
model.matrix( ~ 0 + factor(time), data = willett) %>% 
  head()
```

--
Could be helpful if time is coded in a more complicated way

---
# Fit the model

```{r }
varcomp <- lmer(opp ~  time + (0 + t0 + t1 + t2 + t3 || id), w)
summary(varcomp)
```


---
# Estimation
* Estimates the variance at each timepoint independently (`||`)

  + In other words, no correlation is estimated
  
```{r }
sundry::pull_residual_vcov(varcomp)[1:4, 1:4]
```

---
# Thinking deeper

How reasonable is it to assume the variance at one time point?


--
In most cases, probably not very.


--
.realbig[BUT]


--
Sometimes we can't reliably estimate the covariances, so this helps simplify our model so it's estimable, even if the assumptions we're making are stronger.

---
# Fully unstructured
* We could estimate separate variance *and* all the covariances - this is just a *really* complicated model

* By default, **lme4** will actually try to prevent you from doing this

--
```{r error = TRUE}
fully_unstructured <- lmer(opp ~  time + 
                             (0 + t0 + t1 + t2 + t3 | id), 
                           data = w)
```

---
# Ignore checks

Number of random effects == observations. 

We can still estimate, just tell the model to ignore this check

--
```{r }
fully_unstructured <- lmer(
  opp ~  time + (0 + t0 + t1 + t2 + t3 | id), 
  data = w,
  control = lmerControl(check.nobs.vs.nRE = "ignore") #<<
) 
```

---
```{r }
arm::display(fully_unstructured)
```

---
# Terminology

* What **lme4** fits by default is generally referred to as an *unstructured* variance-covariance matrix. 

* This means the random effect variances and covariances are all estimated

* The previous model I am referring to as *fully* unstructured, because we estimate seperate variances at each time point

* Some might not make this distinction, so just be careful when people use this term

---
class: inverse-orange middle
# Autoregressive

---
# Autoregressive
* There are many types of autoregressive structures
  + If you took a class on time-series data you'd learn about others
  
* What we'll talk about is referred to as an AR1 structure

* Variances (on the diagonal) are constant

* Includes constant "band-diagonals"


---
# Autoregressive

$$
\begin{equation}
  \boldsymbol{\Sigma_r} = \left[
    \begin{array}{cccc}
      \sigma^2 & \sigma^2\rho & \sigma^2\rho^2 & \sigma^2\rho^3 \\
      \sigma^2\rho & \sigma^2 & \sigma^2\rho & \sigma^2\rho^2 \\
      \sigma^2\rho^2 & \sigma^2\rho & \sigma^2 & \sigma^2\rho \\
      \sigma^2\rho^3 & \sigma^2\rho^2 & \sigma^2\rho & \sigma^2
    \end{array}
  \right]
\end{equation}
$$
* Each band is forced to be lower than the prior by a constant fraction
  + estimated autocorrelation parameter $\rho$. The error variance is multiplied by $\rho$ for the first diagonal, by $\rho^2$ for the second, etc.

* Uses only two variance components


---
# Fit

First load **nlme**

```{r message = FALSE}
library(nlme)
```


--
We'll use the `gls()` function. The interface is, overall, fairly similar to **lme4**

--
```{r }
ar <- gls(opp ~ time, 
          data = willett,
          correlation = corAR1(form = ~ 1|id))
```

---
# Summary
Notice, you're not  estimating random effect variances, just an alternative residual covariance structure
```{r }
summary(ar)
```

---
# Extract composite residual

```{r }
cm_ar <- corMatrix(ar$modelStruct$corStruct) # all of them
cr_ar <- cm_ar[[1]] # just the first (they're all the same)
cr_ar
```


--

Multiply the correlation matrix by the model residual variance to get the covariance matrix

```{r }
cr_ar * sigma(ar)^2
```

---
# Confirming calculations

```{r}
sigma(ar)^2
sigma(ar)^2*0.8249118
sigma(ar)^2*0.8249118^2
sigma(ar)^2*0.8249118^3
```

---
# Heterogenous autoregressive
* Same as autorgressive but allows each variance to differ

* Still one $\rho$ estimated

  + Same "decay" across band diagonals

* Band diagonals no longer equivalent, because different variances

---
# Heterogenous autoregressive

$$
\begin{equation}
  \boldsymbol{\Sigma_r} = \left[
    \begin{array}{cccc}
      \sigma^2_1 & \sigma_1\sigma_2\rho & \sigma_1\sigma_3\rho^2 & \sigma_1\sigma_4\rho^2 \\
      \sigma_2\sigma_1\rho & \sigma^2_2 & \sigma_2\sigma_3\rho & \sigma_2\sigma_4\rho^2 \\
      \sigma_3\sigma_1\rho^2 & \sigma_3\sigma_2\rho & \sigma^2_3 & \sigma_3\sigma_4\rho \\
      \sigma_4\sigma_1\rho^3 & \sigma_4\sigma_2\rho^2 & \sigma_4\sigma_3\rho & \sigma^2_4
    \end{array}
  \right]
\end{equation}
$$

---
# Fit
Note - `varIdent` specifies different variances for each wave (variances of the [identity matrix](https://en.wikipedia.org/wiki/Identity_matrix))

```{r }
har <- gls(
  opp ~ time,
  data = willett,
  correlation = corAR1(form = ~ 1|id),
  weights = varIdent(form = ~1|time)
)
```

---
# Summary
```{r }
summary(har)
```

---
# Extract/compute composite residual

The below is fairly complicated, but you can work it out if you go line by line

```{r }
cm_har <- corMatrix(har$modelStruct$corStruct)[[1]] 
var_struct <- har$modelStruct$varStruct
vars <- coef(var_struct, unconstrained = FALSE, allCoef = TRUE)
vars <- matrix(vars, ncol = 1)

cm_har * sigma(har)^2 * 
  (vars %*% t(vars)) # multiply by a mat of vars
```

---
# Toeplitz

* Constant variance

* Has identical band-diagonals, like autoregressive

* Relaxes assumption of each band being a parallel by a common fraction of the prior band

  + Each band determined empirically by the data


--
A bit of a compromise between prior two

---
# Toeplitz

$$
\begin{equation}
  \boldsymbol{\Sigma_r} = \left[
    \begin{array}{cccc}
      \sigma^2 & \sigma^2_1 & \sigma^2_2 & \sigma^2_3 \\
      \sigma^2_1 & \sigma^2 & \sigma^2_1 & \sigma^2_2 \\
      \sigma^2_2 & \sigma^2_1 & \sigma^2 & \sigma^2_1 \\
      \sigma^2_2 & \sigma^2_2 & \sigma^2_1 & \sigma^2
    \end{array}
  \right]
\end{equation}
$$

--
## Fit

```{r }
toep <- gls(opp ~ time, 
            data = willett,
            correlation = corARMA(form = ~ 1|id, p = 3))
```

---
# Summary
```{r }
summary(toep)
```

---
# Extract/compute composite residual
Same as with autoregressive - just multiply the correlation matrix by the residual variance

```{r }
cr_toep <- corMatrix(toep$modelStruct$corStruct)[[1]] 
cr_toep * sigma(toep)^2
```


---
# Comparing fits

```{r }
library(performance)
compare_performance(ar, har, toep, w1, varcomp, fully_unstructured,
                    metrics = c("AIC", "BIC"),
                    rank = TRUE) %>% 
  as_tibble()
```

We have slight evidence here that the Toeplitz structure fits better than the standard unstructured version, which was slightly better than the autoregressive model.

---
# gls models

```{r echo = FALSE}
modelsummary::modelsummary(list(
  "Autoregressive (AR)" = ar, 
  "Heterogeneous AR" = har,
  "Toeplitz" = toep
  )
)
```

---
# lme4 models

```{r echo = FALSE}
modelsummary::modelsummary(list(
  "Standard" = w1,
  "Var Comp" = varcomp,
  "Fully Unstruc" = fully_unstructured
  )
)
```

---
class: inverse-red middle
# Stepping back
### Why do we care about all of this?

---
# Overfitting

* If a simpler model fits the data just as well, it should be preferred

* Models that are overfit have "learned too much" from the data and won't generalize well

* Can think of it as fitting to the errors in the data, rather than the "true" patterns found in the population

---
# Convergence issues

* As your models get increasingly complicated, you're likely to run into convergence issues. 

* Simplifying your residual variance-covariance structure may help 
  + Keeps the basic model structure intact


--
Aside - see [here](http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#troubleshooting) for convergence troubleshooting with **lme4**. The `allFit()` function is often helpful but very computationally intensive.


---
# Summary
* As is probably fairly evident from the code - there are many more structures you could explore. However, most are not implemented through **lme4**.

* Simplifying the residual variance-covariance can sometimes lead to better fitting models

* May also be helpful with convergence and avoid overfitting

---
class: inverse-green middle
# Now
## Homework 2
### Next time: Modeling growth (part 1)
