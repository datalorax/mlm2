---
title: "An introduction to Bayesian estimation"
author: "Daniel Anderson "
date: "Week 6"
output:
  xaringan::moon_reader:
    css: ["default", "new.css"]
    lib_dir: libs
    nature:
      navigation:
        scroll: false
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "https://platform.twitter.com/widgets.js"
    includes:
      in_header: "load-feather.html"
---

```{r include = FALSE}
source(here::here("static", "slides", "slide-setup.R"))
xaringanExtra::use_clipboard()
library(tidyverse)
theme_set(theme_minimal(25))
knitr::opts_chunk$set(fig.width = 13, message = FALSE)
update_geom_defaults('path', list(size = 2))
update_geom_defaults('point', list(size = 5))
update_geom_defaults('text', list(size = 9))
```

`r setup("w6p1")`

---
# Agenda

* Introduce Bayes theorem

* Go through an example with estimating a mean

* Discuss Bayes in the context of regression modeling

* Implementation in multilevel models in R with [**{brms}**](https://github.com/paul-buerkner/brms)

* Model diagnostics and checking for convergence

---
# A disclaimer
* There is **no** chance we'll really be able to do Bayes justice in this class

* The hope for today is that you'll get an introduction

* By the end you should be able to fit the models you already can, but in a Bayes framework

* Hopefully you also recognize the tradeoffs, and potential extensions

---
class: inverse-blue middle
# Bayes theorem

---
# In equation form
You'll see this presented many different ways, perhaps mostly commonly as

$$
p(B \mid A) = \frac{ p(A \mid B) \times p(B)}{p(A)}
$$
where $\mid$ is read as "given" and $p$ is the probability

--

I prefer to give $A$ and $B$ more meaningful names

--

$$
p(\text{prior} \mid \text{data}) = \frac{ p(\text{data} \mid \text{prior}) \times p(\text{prior})}{\text{data}}
$$

---
# A real example
Classifying a student with a learning disability. We want to know 

$$
p(\text{LD} \mid \text{Test}_p) = \frac{ p(\text{Test}_p \mid \text{LD}) \times p(\text{LD})}{\text{Test}_p}
$$
--
Notice, this means we need to know:

--

* True positive rate of the test, $p(\text{Test}_p \mid \text{LD})$


--

* Base rate for learning disabilities, $p(\text{LD})$


--
* Base rate for testing positive, $\text{Test}_p$

---
# Estimating
If we have these things, we can estimate the probability that a student has a learning disability, given a positive test.


--
## Let's assume: 
* $p(\text{Test}_p \mid \text{LD}) = 0.90$

* $p(\text{LD}) = 0.10$

* $\text{Test}_p = 0.20$

---

$$
p(\text{LD} \mid \text{Test}_p) = \frac{ .90 \times .10}{.20}
$$


--
$$
p(\text{LD} \mid \text{Test}_p) = 0.45
$$


--
A bit less than you might have expected? Probability is hard...


--
When we see things like "90% true positive rate" we want to interpret it as $p(\text{LD} \mid \text{Test}_p)$, when it's actually $p(\text{Test}_p \mid \text{LD})$

---
# Pieces
If we know all the pieces, we can estimate Bayes theorem directly. 


--
![](https://media2.giphy.com/media/2ZVrNVOtaM2q1zluYs/giphy.gif)

Unfortunately this is almost never the case...

---
# Alternative view

$$
\text{updated beliefs} = \frac{\text{likelihood of data} \times \text{prior information}}{\text{average likelihood}}
$$

--
How do we calculate the likelihood of the data? We have to assume some distribution.

---
# Example with IQ

.footnote[This example borrowed from [TJ Mahr](https://cdn.rawgit.com/tjmahr/Psych710_BayesLecture/55f446a0/bayes_slides.html)]

```{r }
#install.packages("carData")
iqs <- carData::Burt$IQbio
iqs
```

--
IQ scores are generally assumed to be generated from a distribution that looks like this:

$$
IQ_i \sim N(100, 15)
$$

--
```{r echo = FALSE, fig.height = 3}
ggplot(data.frame(x = c(60, 140)), aes(x)) +
  stat_function(fun = ~dnorm(., 100, 15))
```

---
# Likelihood
What's the likelihood of a score of 80, assuming this distribution?

--
```{r echo = FALSE, fig.height = 6}
ggplot(data.frame(x = c(60, 140)), aes(x)) +
  stat_function(fun = ~dnorm(., 100, 15)) +
  geom_point(aes(y = y),
             data.frame(x = 80, y = dnorm(80, 100, 15)),
             color = "magenta") +
  geom_segment(x = 80, xend = 80, y = 0, yend = dnorm(80, 100, 15),
               linetype = "dashed",
               color = "magenta") +
  geom_segment(x = -Inf, xend = 80, 
               y = dnorm(80, 100, 15), yend = dnorm(80, 100, 15),
               linetype = "dashed",
               color = "magenta")
```

--

```{r }
dnorm(80, mean = 100, sd = 15)
```

---
# Likelihood of the data
We sum the likelihood to get the overall likelihood of the data. However, this leads to very small numbers. Computationally, it's easier to sum the *log* of these likelihoods.

--
```{r }
dnorm(iqs, mean = 100, sd = 15, log = TRUE)
```

--
```{r }
sum(dnorm(iqs, mean = 100, sd = 15, log = TRUE))
```

---
# Alternative distributions

What if we assumed the data were generated from an alternative distribution, say $IQ_i \sim N(115, 5)$?

--
```{r }
sum(dnorm(iqs, mean = 115, sd = 5, log = TRUE))
```

--
The value is *much* lower. In most models, we are estimating $\mu$ and $\sigma$, and trying to find values that *maximize* the sum of the log likelihoods.

---
# Visually

The real data generating distribution

```{r echo = FALSE}
iq_likelihood <- data.frame(x = iqs, y = dnorm(iqs, 100, 15))

ggplot(data.frame(x = c(60, 140)), aes(x)) +
  stat_function(fun = ~dnorm(., 100, 15)) +
  geom_point(aes(y = y),
             iq_likelihood,
             color = "magenta") +
  geom_segment(aes(x = x, xend = x, y = 0, yend = y),
               iq_likelihood,
               linetype = "dashed",
               color = "magenta")
```

---
# Visually

The poorly fitting one

```{r echo = FALSE}
iq_likelihood2 <- data.frame(x = iqs, y = dnorm(iqs, 120, 5))

ggplot(data.frame(x = c(60, 140)), aes(x)) +
  stat_function(fun = ~dnorm(., 120, 5)) +
  geom_point(aes(y = y),
             iq_likelihood2,
             color = "magenta") +
  geom_segment(aes(x = x, xend = x, y = 0, yend = y),
               iq_likelihood2,
               linetype = "dashed",
               color = "magenta")
```

---
# Non-Bayesian
In a frequentist regression model, we would find parameters that *maximize* the likelihood. Note - the distributional mean is often conditional.


--
This is part of why I've come to prefer notation that emphasizes the data generating process. 

---
# Example
I know we've talked about this before, but a simple linear regression model like this

```{r }
m <- lm(IQbio ~ class, data = carData::Burt)
```

--
is generally displayed like this


```{r echo = FALSE}
equatiomatic::extract_eq(m, font_size = "normalsize")
```


--
But we could display the same thing like this
$$
\begin{align}
\operatorname{IQbio} &\sim N(\widehat{\mu}, \widehat{\sigma}) \\
\widehat{\mu} = \alpha &+ \beta_{1}(\operatorname{class}_{\operatorname{low}}) + \beta_{2}(\operatorname{class}_{\operatorname{medium}})
\end{align}
$$

---
class: inverse-red middle

# Priors

---
# Bayesian posterior

$$
\text{posterior} = \frac{ \text{likelihood} \times \text{prior}}{\text{average likelihood}}
$$


--
The above is how we estimate with Bayes. 


--
In words, it states that our updated beliefs (posterior) depend on the evidence from our data (likelihood) and our prior knowledge/conceptions/information (prior).


--
Our prior will shift in accordance with the evidence from the data

---
# Basic example
Let's walk through a basic example where we're just estimating a mean. We'll assume we somehow magically know the variance. Please follow along.


--
First, generate some data

```{r }
set.seed(123)
true_data <- rnorm(50, 5, 1)
```

---
# Grid search
We're now going to specify a grid of possible means for our data. Let's search anywhere from -3 to 8 in 0.1 intervals.

--
```{r }
grid <- tibble(possible_mean = seq(-3, 8, 0.1))
```

--

Next, we'll specify a *prior distribution*. That is - how likely do we *think* each of these possible means are?


--
Let's say our best guess is $mu = 2$. Values on either side of $2$ should be less likely.  
--
```{r }
prior <- dnorm(grid$possible_mean, mean = 2, sd = 1)
```

---
# Plot our prior

```{r }
grid %>% 
  mutate(prior = prior) %>% 
  ggplot(aes(possible_mean, prior)) +
  geom_line()
```

Note that the *strength* of our prior depends on the standard deviation

This would be our best guess as to where the data would fall *before* observing the data.

---
# Look at other priors

```{r fig.height = 6}
grid %>% 
  mutate(prior1 = dnorm(possible_mean, mean = 2, sd = 1),
         prior2 = dnorm(grid$possible_mean, mean = 2, sd = 2),
         prior3 = dnorm(grid$possible_mean, mean = 2, sd = 3)) %>% 
  ggplot(aes(possible_mean)) +
  geom_line(aes(y = prior1)) + 
  geom_line(aes(y = prior2), color = "cornflowerblue") + 
  geom_line(aes(y = prior3), color = "firebrick")  
```

---
# Set prior
Let's go with a fairly conservative prior, with $\mu = 2, \sigma = 3$.

---
# Observe 1 data point
```{r }
grid %>% 
  mutate(prior = dnorm(possible_mean, 2, 1),
         likelihood = dnorm(true_data[1], possible_mean, 1))
```

---
# Multiply & scale

```{r }
point_one <- grid %>% 
  mutate(prior = dnorm(possible_mean, 2, 1),
         likelihood = dnorm(true_data[1], possible_mean, 1),
         unscaled = prior * likelihood,
         posterior = unscaled / sum(unscaled))
point_one
```


---
# Plot

```{r }
point_one %>% 
  select(-unscaled) %>% 
  pivot_longer(-possible_mean) %>% 
  ggplot(aes(possible_mean, value)) +
  geom_line(aes(color = name))
```


---
# Second point

```{r }
point_two <- point_one %>% 
  mutate(
    previous = posterior,
    likelihood = dnorm(true_data[2], one_point$possible_mean, 1),
    unscaled = prior * likelihood,
    posterior = unscaled / sum(unscaled))
point_two
```


---
# Plot

```{r }
point_two %>% 
  select(-unscaled) %>% 
  pivot_longer(-possible_mean) %>% 
  ggplot(aes(possible_mean, value)) +
  geom_line(aes(color = name))
```


---
# Third point

```{r }
point_three <- point_two %>% 
  mutate(
    previous = posterior,
    likelihood = dnorm(true_data[3], one_point$possible_mean, 1),
    unscaled = prior * likelihood,
    posterior = unscaled / sum(unscaled))
point_three
```


---
# Plot

```{r }
point_three %>% 
  select(-unscaled) %>% 
  pivot_longer(-possible_mean) %>% 
  ggplot(aes(possible_mean, value)) +
  geom_line(aes(color = name))
```

----
# Loop

```{r }

```


---
# Compute likelihood
Let's compute the likelihood of the data under each possible mean (assuming we know the variance)

--
Note - the code is a little complicated here, but I'll explain

```{r }
grid %>% 
  rowwise() %>% 
  mutate(sum_ll = sum(dnorm(true_data, possible_mean, log = TRUE)))
```

---
# Get most likely mean

```{r }
grid %>% 
  rowwise() %>% 
  mutate(sum_ll = sum(dnorm(true_data, possible_mean, log = TRUE))) %>% 
  ungroup() %>% 
  filter(sum_ll == max(sum_ll))
```

---
# Plot prior & likelihood

```{r }
grid <- grid %>% 
  mutate(prior = dnorm(possible_mean, mean = 2, sd = 3),
         likelihood = dnorm(possible_mean, mean = 5.2, sd = 1))
grid
```

---
# Plot prior & likelihood

```{r }
ggplot(grid, aes(possible_mean, prior)) +
  geom_line() +
  geom_line(aes(y = likelihood),
            color = "cornflowerblue")
```

---
# Compute posterior

```{r }
grid %>% 
  mutate(posterior = (likelihood * prior) / mean(likelihood))
```

---
# Plot

```{r }
grid %>% 
  mutate(posterior = (likelihood * prior) / mean(likelihood)) %>% 
  pivot_longer(-possible_mean, names_to = "Distribution") %>% 
  ggplot(aes(possible_mean, value)) +
  geom_line(aes(color = Distribution)) +
  scale_color_brewer(palette = "Set2")
```

---
# Finding the mean
```{r }
grid %>% 
  mutate(posterior = (likelihood * prior) / mean(likelihood)) %>% 
  filter(posterior == max(posterior))
```

---
# Change our prior
Let's try again with a tighter prior

```{r eval = FALSE}
grid <- grid %>% 
  mutate(prior = dnorm(possible_mean, mean = 2, sd = 0.5),
         likelihood = dnorm(possible_mean, mean = 5.2, sd = 1))

ggplot(grid, aes(possible_mean, prior)) +
  geom_line() +
  geom_line(aes(y = likelihood),
            color = "cornflowerblue")
```

---

```{r echo = FALSE}
grid <- grid %>% 
  mutate(prior = dnorm(possible_mean, mean = 2, sd = 0.5),
         likelihood = dnorm(possible_mean, mean = 5.2, sd = 1))

ggplot(grid, aes(possible_mean, prior)) +
  geom_line() +
  geom_line(aes(y = likelihood),
            color = "cornflowerblue")
```


---
```{r }
grid %>% 
  mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) %>% 
  pivot_longer(-possible_mean, names_to = "Distribution") %>% 
  ggplot(aes(possible_mean, value)) +
  geom_line(aes(color = Distribution)) +
  scale_color_brewer(palette = "Set2")
```

---
# Different prior distribution

```{r }
grid <- grid %>% 
  mutate(prior = dunif(possible_mean, min = 0, max = 8),
         likelihood = dnorm(possible_mean, mean = 5.2, sd = 1))

ggplot(grid, aes(possible_mean, prior)) +
  geom_line() +
  geom_line(aes(y = likelihood),
            color = "cornflowerblue")
```

---
```{r }
grid %>% 
  mutate(posterior = (likelihood * prior) / mean(likelihood)) %>% 
  pivot_longer(-possible_mean, names_to = "Distribution") %>% 
  ggplot(aes(possible_mean, value)) +
  geom_line(aes(color = Distribution)) +
  scale_color_brewer(palette = "Set2")
```

---
# Taking a step back
* The purpose of the prior is to include *what you already know* into your analysis

* The strength of your prior should depend on your prior research

* Larger samples will overwhelm priors quicker

* Think through the lens of updating your prior beliefs

* This whole framework is quite different, but also gives us a lot of advantages in terms of probability interpretation, as we'll see

---
class: inverse-blue middle
# Bayes for regression

---
class: inverse-blue middle

# Implementation with {brms}

.center[
![](https://github.com/paul-buerkner/brms/raw/master/man/figures/brms.png)
]

---
# What is it?
* **b**ayesian **r**egression **m**odeling with **s**tan

* Uses [stan](https://mc-stan.org/) as the model backend - basically writes the model code for you then sends it to stan

* Allows model syntax similar to **lme4** 

* Simple specification of priors - defaults are flat

* Provides many methods for post-model fitting inference

---
# Fit a basic model
Let's start with the default (uninformative) priors, and fit a standard, simple-linear regression model

```{r }
library(brms)
sleep_m0 <- brm(Reaction ~ Days, data = lme4::sleepstudy)
summary(sleep_m0)
```

---
# View fixed effect
Let's look at our estimated relation between `Days` and `Reaction`

--
```{r }
conditional_effects(sleep_m0)
```


---
# Wrong model
Of course, this is the wrong model, we have a multilevel structure

```{r echo = FALSE, message = FALSE}
ggplot(lme4::sleepstudy, aes(Days, Reaction)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~Subject) +
  theme_minimal(15)
```

---
# Multilevel model
Notice the syntax is essentially equivalent to **lme4**

```{r }
sleep_m1 <- brm(Reaction ~ Days + (Days | Subject), data = lme4::sleepstudy)
summary(sleep_m1)
```

---
# Fixed effect
The uncertainty has increased

```{r }
conditional_effects(sleep_m1)
```

---
# Checking your model

```{r }
kidney_m0 <- brm(time ~ age + sex, data = kidney, family = gaussian())
pp_check(kidney_m0, type = "ecdf_overlay")
```

---
# Fixing this
We need to change the assumptions of our model - specifically that the outcome is not normally distributed

```{r }
kidney_m1 <- brm(time ~ age + sex, data = kidney, family = poisson())
pp_check(kidney_m1, type = "ecdf_overlay")
```

---
```{r }
kidney_m2 <- brm(time ~ age + sex, data = kidney, family = Gamma("log"))
pp_check(kidney_m2, type = "ecdf_overlay")
```


---
# Specifying priors
Let's sample from *only* our priors to see what kind of predictions we get.

Here, we're specifying that our beta coefficient prior is $\beta \sim N(0, 0.5)$ 

```{r }
kidney_m3 <- brm(
  time ~ age + sex, 
  data = kidney,
  family = Gamma("log"),
  prior = prior(normal(0, 0.5), class = "b"),
  sample_prior = "only"
)
kidney_m3
```

---
# Prior predictions
```{r echo = FALSE}
kidney %>% 
  tidybayes::add_fitted_draws(kidney_m3) %>% 
  ggplot(aes(time, .value)) +
  geom_point(alpha = 0.1, stroke = 0) +
  facet_wrap(~sex) +
  scale_y_log10(labels = scales::comma)
```

---
# Why?
It seemed like our prior was fairly tight

--
The exploding prior happens because of the log transformation

* Age is coded in years

* Imagine a coef of 1 (2 standard deviations above our prior)

* Prediction for a 25 year old would be exp(25) = `r exp(25)`

---
# A note on prior specifications
* It's hard

* I don't have a ton of good advice

* Be particularly careful when you're using distributions that have anything other than an identity link (e.g., log link, as we are here)

---
# other notes
The posterior is the distribution of the parameters - given the data

Basically the distribution of what we don't know, but are interested in (the model parameters), given what we know or have observed (the data)

Gives a complete understanding of modeling uncertainty

We can do lots of stuff with this that's hard to get otherwise

Missing values can be treated as unknown variables (parameters)

---
# Advantages to Bayes
* Opportunity to incorporate prior knowledge into the modeling process (you don't really *have* to - could just set wide priors)

* Natural interpretation of uncertainty

* Can often allow you to estimate models that are difficult if not impossible with frequentist methods


--

## Disadvatages

* Generally going to be slower in implementation

* You may run into pushback from other