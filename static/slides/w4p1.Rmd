---
title: "Review & Intro to GH Notation"
author: "Daniel Anderson "
date: "Week 4"
output:
  xaringan::moon_reader:
    css: ["default", "new.css"]
    lib_dir: libs
    nature:
      navigation:
        scroll: false
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "https://platform.twitter.com/widgets.js"
    includes:
      in_header: "load-feather.html"
---

```{r include = FALSE, results = "asis"}
source(here::here("static", "slides", "slide-setup.R"))
xaringanExtra::use_clipboard()
library(tidyverse)
theme_set(theme_minimal(25))
knitr::opts_chunk$set(fig.width = 13)
update_geom_defaults('errorbarh', list(size = 2))
update_geom_defaults('path', list(size = 3))
update_geom_defaults('point', list(size = 5))
update_geom_defaults('text', list(size = 9))
```

`r setup("w4p1")`

---
# Agenda

* Review Homework 1

* Review the most important parts of Week 3 content

* Discuss Gelman and Hill notation - contrast with Raudenbush and Bryk

* Unstructured VCV Matrices and alternatives


---
# Learning Objectives

* Understand at least the basics of the GH notation and why I view it as preferable

* Gain a deeper understanding of how the residual structure is different in multilevel models

* Understand that there are methods for changing the residual structure, and understand when and why this might be preferable

* Have a basic understanding of implementing alternative methods

---
class: inverse-blue middle

# Review Homework 1

---
class: inverse-red middle

# Review Week 3 content

---
# Data and model

```{r message = FALSE}
library(tidyverse)
library(lme4)

popular <- read_csv(here::here("data", "popularity.csv"))

m <- lmer(popular ~ extrav + (extrav|class), popular,
          control = lmerControl(optimizer = "bobyqa"))
```

`extrav` is a measure of extraversion.

What is this model fitting, in plain English?

---
# Model summary
```{r }
arm::display(m)
```

---
# Let's walk through
* By way of thinking through it, let's compare to simple linear regression

--
```{r }
slr <- lm(popular ~ extrav, popular)
arm::display(slr)
```

---
# Visually
```{r message = FALSE}
ggplot(popular, aes(extrav, popular)) +
  geom_point() +
  geom_smooth(se = FALSE, method = "lm")
```

---
# Making predictions

```{r echo = FALSE}
equatiomatic::extract_eq(slr, use_coef = TRUE)
```

--
Scores of 0 to 5

$$
\operatorname{\widehat{popular}} = 3.27 + 0.35 \times 0 = 3.27
$$

$$
\operatorname{\widehat{popular}} = 3.27 + 0.35 \times 1 = 3.62 
$$

$$
\operatorname{\widehat{popular}} = 3.27 + 0.35 \times 2 = 3.97 
$$
$$
\operatorname{\widehat{popular}} = 3.27 + 0.35 \times 3 = 4.32 
$$

$$
\operatorname{\widehat{popular}} = 3.27 + 0.35 \times 4 = 4.67 
$$

$$
\operatorname{\widehat{popular}} = 3.27 + 0.35 \times 5 = 5.02
$$


---
# Now for the mlm
It's more complicated now for a couple of reasons

* Each `class` has their own intercept and slope. Which class is the student in?

* What if we want to make a prediction for someone outside our sample?

--
Let's walk through 4 predictions

---
# Sample preds
```{r }
sample_preds <- popular %>% 
  group_by(class) %>% 
  slice(1) %>% 
  ungroup() %>% 
  slice(1:4)

sample_preds
```

---
# Coefficients
```{r echo = FALSE}
equatiomatic::extract_eq(m, use_coef = TRUE, mean_separate = TRUE)
```

---
# Grab params
Fixed effects
```{r }
f <- fixef(m)
f
```

--
classroom deviations

```{r }
r <- ranef(m)
r
```

---
# Predictions depend on classroom

### Fixed effect part
Works just like simple linear regression

--
```{r }
sample_preds[1, ]
```

$$
\operatorname{\widehat{popular}} = 2.46 + 0.49 \times 5 = 4.91
$$
--

```{r }
f[1] + f[2]*5
```



---
# Random effects
We now have to add in the random effects *for the corresponding classroom*.

--
```{r }
head(r$class)
```

--
$$
\operatorname{\widehat{popular}} = (2.46 + 0.34) + (0.49 + -0.03) \times 5 = 5.10
$$
---
# In code
```{r }
class1 <- r$class[1, ]
class1
(f[1] + class1[1]) + (f[2] + class1[2])*5
```


---
# Predictions
```{r }
sample_preds
head(r$class, n = 4)
fixef(m)
```

---

$$
\operatorname{\widehat{popular}} = (2.46 + 0.34) + (0.49 + -0.03) \times 5 = 5.10
$$

$$
\operatorname{\widehat{popular}} = (2.46 + -1.18) + (0.49 + 0.10) \times 8 = 6.00
$$
$$
\operatorname{\widehat{popular}} = (2.46 + -0.63) + (0.49 + 0.06) \times 5 = 4.58
$$

$$
\operatorname{\widehat{popular}} = (2.46 + 1.09) + (0.49 + -0.10) \times 3 = 4.72
$$
--
```{r }
predict(m, newdata = sample_preds)
```

--
### What if we want to make a prediction outside of our classrooms?

--
Fixed effects only

$$
\operatorname{\widehat{popular}} = 2.46 + 0.49 \times \operatorname{extraversion} 
$$
---
# Plotting
We can use the `expand.grid()` function to create different conditions.

--
Let's compare slopes across the first five classrooms

```{r }
conditions <- expand.grid(extrav = 1:10, class = 1:5)
```

.pull-left[
```{r }
head(conditions)
```
]

.pull-right[
```{r }
tail(conditions)
```
]

---
# Make predictions
```{r }
conditions %>% 
  mutate(model_pred = predict(m, newdata = conditions))
```

---
# Plot

```{r }
conditions %>% 
  mutate(model_pred = predict(m, newdata = conditions)) %>% 
  ggplot(aes(extrav, model_pred)) +
  geom_line(aes(group = class))
```

---
# One more quick example
Model an interaction
```{r }
m2 <- lmer(popular ~ extrav*sex + (extrav|class), popular,
          control = lmerControl(optimizer = "bobyqa"))
```

---
# Model summary
```{r }
arm::display(m2)
```

---
# Marginal effect
Let's look at the interaction between extraversion and sex

```{r }
conditions2 <- expand.grid(extrav = 1:10, 
                           sex = c("girl", "boy"), 
                           class = 0) %>% 
  mutate(pred = predict(m2, 
                        newdata = ., 
                        allow.new.levels = TRUE))
conditions2
```

---
# Plot
```{r }
ggplot(conditions2, aes(extrav, pred)) +
  geom_line(aes(color = sex))
```

---
class: inverse-red middle
# Questions?

---
class: inverse-blue middle

# Notation
### Introducing the Gelman and Hill notation

---
# Standard regression

Imagine we have a model like this

```{r }
m <- lm(mpg ~ disp + hp + drat, data = mtcars)
```



--
We would probably display this model like this


$$
\begin{equation}
\operatorname{mpg} = \alpha + \beta_{1}(\operatorname{disp}) + \beta_{2}(\operatorname{hp}) + \beta_{3}(\operatorname{drat}) + \epsilon
\end{equation}
$$

--
What we often don't show, is the distributional assumption of the residuals

$$
\epsilon \sim N\left(0, \sigma \right)
$$

---
# A different view

The model on the previous slide could also be displayed  like this

$$
\begin{aligned}
\hat{y} &= \alpha + \beta_{1}(\operatorname{disp}) + \beta_{2}(\operatorname{hp}) + \beta_{3}(\operatorname{drat}) \\\
\operatorname{mpg} &\sim N\left(\hat{y}, \sigma \right)
\end{aligned}
$$

--
This makes the distributional assumptions clearer


--
Each `mpg` value is assumed generated from a normal distribution, with a mean structure according to $\hat{y}$, and an unknown standard deviation, $\sigma$.

---
# Simulate
### I'm not expecting you to follow along here

If we have a solid understanding of the distributional properties, we can simulate new data from the model


--
First let's set some population parameters

```{r }
n <- 1000
intercept <- 100
b1 <- 5
b2 <- -3
b3 <- 0.5
sigma <- 4.5
```

---
# Simulate

Next create some variables. The standard deviations relate to the standard errors - more variance in the predictor leads to lower standard errors.

```{r }
set.seed(123)
x1 <- rnorm(n, sd = 1)
x2 <- rnorm(n, sd = 2)
x3 <- rnorm(n, sd = 4)
```

--
## Create y-hat

```{r }
yhat <- intercept + b1*x1 + b2*x2 + b3*x3
```

---
# Generate data & test
```{r }
sim <- rnorm(n, yhat, sigma)
summary(lm(sim ~ x1 + x2 + x3))
```

---
# Generalizing
We can generalize this same basic approach to multilevel models

This is helpful because the error structure is more complicated

Using this approach helps us better understand the distributional assumptions of our model

---
# Simple example
I know we hate the HSB data but bear with me for a minute.

Consider this simple model
```{r message = FALSE}
library(lme4)
library(equatiomatic)
hsb_m0 <- lmer(math ~ ses + (1|sch.id), data = hsb)
```

---
# R&B
In Raudenbush and Bryk notation, the model on the prior slide would look like this

$$
\begin{aligned}
\text{math}_{ij} &= \beta_{0j} + \beta_{1j}(\text{ses}) + e_{ij} \\\
\beta_{0j} &= \gamma_{00} + u_{0j} \\\
\beta_{1j} &= \gamma_{10}
\end{aligned}
$$

--

.pull-left[

Generally, the distributional part is omitted, which in this case is

$$
\begin{aligned}
&E\left(e_{ij} \right) = 0, \text{Var}\left(e_{ij} \right) = \sigma^2 \\\
&E\left(u_{0j} \right) = 0, \text{Var}\left(u_{0j} \right) = \tau_{00}
\end{aligned}
$$
]


--

.pull-right[

Put differently

$$
\begin{aligned}
e_{ij} &\sim N\left(0, \sigma^2 \right) \\\
u_{0j} &\sim N\left(0, \tau_{00} \right)
\end{aligned}
$$

]

---
# G&H
In Gelman & Hill notation, this same model can be communicated as

```{r message = FALSE, echo = FALSE}
extract_eq(hsb_m0)
```

--
This notation communicates the distributional assumptions


--
We can also still easily see what levels the predictors are at


--
It does look a little more complex, but it's not hiding anything


--
If you properly understand the notation, you can simultate data assuming this data generating process (which we'll do later)

---
# Bonus
It works really well to communicate model results

```{r echo = FALSE}
extract_eq(hsb_m0, use_coef = TRUE)
```

--
### Extra bonus!
You can use equatiomatic to give you the model formula. The above was generated with `extract_eq(hsb_m0, use_coef = TRUE)`

---
# Quick simulation
We'll go over this in more detail later, but I want to give you the general idea.

First, set some parameters
```{r }
j <- 30 # 30 schools
nj <- 50 # 50 students per school
```

Next, simulate the school distribution

```{r}
# School distribution
a_j <- rnorm(j, 0, 2.18)
```

---
For each school, simulate nj obs from the level 1 model, adding in the school deviation

There are lots of ways to do this - I'm using a `for()` loop here in an effort to be transparent

```{r }
school_scores <- vector("list", j)
ses <- vector("list", j)

for(i in 1:j) {
  ses[[i]] <- rnorm(nj)
  school_scores[[i]] <- rnorm(nj, 
                              12.66 + 2.39*ses[[i]] + a_j[i], 
                              6.09)
}
```

---
# Put in a df

```{r }
sim_df <- data.frame(
  scid = rep(1:j, each = nj),
  ses = unlist(ses),
  score = unlist(school_scores)
)
head(sim_df)
```

---
# Test it out
```{r }
sim_m0 <- lmer(score ~ ses + (1|scid), data = sim_df)
summary(sim_m0)
```


---
# Expanding the model
Let's add a school-level predictor

--

```{r }
hsb_m1 <- lmer(math ~ ses + sector + (1|sch.id), data = hsb)
```

--
```{r }
extract_eq(hsb_m1)
```

---
# Add in a random slope

.small80[
```{r }
hsb_m2 <- lmer(math ~ ses + sector + (ses|sch.id), data = hsb)
extract_eq(hsb_m2)
```
]

---
# Include interaction
Include `sector` as a predictor of the relation between `ses` and `math`

.small80[
```{r }
hsb_m3 <- lmer(math ~ ses * sector + (ses|sch.id), data = hsb,
               control = lmerControl(optimizer = "nmkbw"))
extract_eq(hsb_m3)
```
]

---
# Even more complicated

This model doesn't actually fit well - I omitted some convergence warnings

.small50[
```{r warning = FALSE, message = FALSE}
hsb_m4 <- lmer(
  math ~ ses * sector + minority + female + meanses + size +
    (ses + minority + female|sch.id), 
  data = hsb
)

extract_eq(hsb_m4)
```

]

---
# Multiple levels
Let's go to a different dataset from equatiomatic

```{r }
head(sim_longitudinal)
```

---
# Four levels
Model doesn't really fit again

.small70[
```{r message = FALSE}
sl_m <- lmer(
  score ~ wave*treatment + group + prop_low +
    (wave|sid) + (wave + treatment| school) + (1|district),
  data = sim_longitudinal
)
extract_eq(sl_m)
```
]
---
class: inverse-blue middle

# Residual structures

---
# Data
[Willett, 1988](https://www.jstor.org/stable/1167368?seq=1#metadata_info_tab_contents)

* $n$ = 35 people
* Each completed a cognitive inventory on "opposites naming"
* At first time point, participants also completed a general cognitive measure

---
# Read in data

```{r message = FALSE}
willett <- read_csv(here::here("data", "willett-1988.csv"))
willett
```

---
# Standard OLS 

* We have four observations per participant. 

* If we fit a standard OLS model, it would look like this

```{r }
bad <- lm(opp ~ time, data = willett)
summary(bad)
```

---
# Assumptions
As we discussed previously, this model looks like this

$$
\operatorname{opp} = \alpha + \beta_{1}(\operatorname{time}) + \epsilon
$$

where

$$
\epsilon \sim \left(0, \sigma \right)
$$

---
# Individual level residuals
We can expand our notation, so it looks like a multivariate normal distribution

$$
\begin{equation}
  \left(
    \begin{array}{c}
      \epsilon_1 \\\
      \epsilon_2 \\\
      \epsilon_3 \\\
      \vdots \\\
      \epsilon_n
    \end{array}
  \right)
\sim MVN
  \left(
    \left[
      \begin{array}{c}
        0\\\
        0\\\
        0\\\
        \vdots \\\
        0
    \end{array}
    \right],
    \left[
      \begin{array}{ccccc}
        \sigma_{\epsilon} & 0 & 0 & \dots & 0 \\\
        0 & \sigma_{\epsilon} & 0 & 0 & 0 \\\
        0 & 0 & \sigma_{\epsilon} & 0 & 0 \\\
        \vdots & 0 & 0 & \ddots & \vdots \\\
        0 & 0 & 0 & \dots & \sigma_{\epsilon}
      \end{array}
    \right]
  \right)
\end{equation}
$$

This is where the $i.i.d.$ part comes in. The residuals are assumed $i$ndependent and $i$dentically $d$istributed.

---
# Multilevel model
Very regularly, there are reasons to believe the $i.i.d.$ assumption is violated. Consider our current case, with 4 time points for each individual.

* Is an observation for one time point for one individual *independent* from the other observations for that individual?


--
* Rather than estimating a single residual variance, we estimate an additional components associated with individuals, leading to a *block* diagonal structure

---
# Block diagonal

.small70[
$$
\begin{equation}
  \left(
    \begin{array}{c}
      \epsilon_{11} \\
      \epsilon_{12} \\
      \epsilon_{13} \\
      \epsilon_{14} \\
      \epsilon_{21} \\
      \epsilon_{22} \\
      \epsilon_{23} \\
      \epsilon_{24} \\
      \vdots \\
      \epsilon_{n1} \\
      \epsilon_{n2} \\
      \epsilon_{n3} \\
      \epsilon_{n4}
    \end{array}
  \right)
\sim
  \left(
    \left[
      \begin{array}{c}
        \\
        0\\
        0\\
        0\\
        0\\
        0\\
        0\\
        0\\
        0\\
        \vdots \\
        0\\
        0\\
        0\\
        0
      \end{array}
    \right],
    \left[
      \begin{array}{{@{}*{13}c@{}}}
      \\
        \sigma_{11} & \sigma_{12} & \sigma_{13} & \sigma_{14} & 0 & 0 & 0 & 0 & \dots  & 0 & 0 & 0 & 0 \\
        \sigma_{21} & \sigma_{22} & \sigma_{23} & \sigma_{24} & 0 & 0 & 0 & 0 & \dots  & 0 & 0 & 0 & 0 \\
        \sigma_{31} & \sigma_{32} & \sigma_{33} & \sigma_{34} & 0 & 0 & 0 & 0 & \dots  & 0 & 0 & 0 & 0 \\
        \sigma_{41} & \sigma_{42} & \sigma_{43} & \sigma_{44} & 0 & 0 & 0 & 0 & \dots  & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & \sigma_{11} & \sigma_{12} & \sigma_{13} & \sigma_{14} & \dots  & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & \sigma_{21} & \sigma_{22} & \sigma_{23} & \sigma_{24} & \dots  & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & \sigma_{31} & \sigma_{32} & \sigma_{33} & \sigma_{34} & \dots  & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & \sigma_{41} & \sigma_{42} & \sigma_{43} & \sigma_{44} & \dots  & 0 & 0 & 0 & 0 \\
        \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots \\
        0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \dots & \sigma_{11} & \sigma_{12} & \sigma_{13} & \sigma_{14} \\
        0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \dots & \sigma_{21} & \sigma_{22} & \sigma_{23} & \sigma_{24} \\
        0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \dots & \sigma_{31} & \sigma_{32} & \sigma_{33} & \sigma_{34} \\
        0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \dots & \sigma_{41} & \sigma_{42} & \sigma_{43} & \sigma_{44}
      \end{array}
    \right]
  \right)
\end{equation}
$$
]


--
Correlations for  off-diagonals estimated


--
Same variance components for all blocks


--
Out-of-block diagonals are still zero

---
# Homogeneity of variance
As mentioned on the previous slide, we assume the same variance components across all student

This is referred to as the homogeneity of variance assumption - although the block (often referred to as the composite residual) may be heteroscedastic and dependent **within** a grouping factor (i.e., people) the entire error structure is repeated identically across units (i.e., people)

---
# Block diagonal
Because of the homogeneity of variance assumption, we can re-express our block diagonal design as follows

$$
\begin{equation}
r \sim N \left(\boldsymbol{0}, 
  \left[
    \begin{array}{ccccc}
      \boldsymbol{\Sigma_r} & \boldsymbol{0} & \boldsymbol{0} & \dots & \boldsymbol{0} \\
      \boldsymbol{0} & \boldsymbol{\Sigma_r} & \boldsymbol{0} & \dots & \boldsymbol{0} \\
      \boldsymbol{0} & \boldsymbol{0} & \boldsymbol{\Sigma_r} &  \dots & \boldsymbol{0} \\
      \vdots & \vdots & \vdots & \ddots & \vdots & \\
      \boldsymbol{0} & \boldsymbol{0} &  \boldsymbol{0} & \dots & \boldsymbol{\Sigma_r}
    \end{array}
  \right]
\right)
\end{equation}
$$
---
# Composite residual
We then define the composite residual, which is common across units

$$
\begin{equation}
  \boldsymbol{\Sigma_r} = \left[
    \begin{array}{cccc}
      \sigma_{11} & \sigma_{12} & \sigma_{13} & \sigma_{14} \\
      \sigma_{21} & \sigma_{22} & \sigma_{23} & \sigma_{24} \\
      \sigma_{31} & \sigma_{32} & \sigma_{33} & \sigma_{34} \\
      \sigma_{41} & \sigma_{42} & \sigma_{43} & \sigma_{44}
    \end{array}
  \right]
\end{equation}
$$

---
# Let's try!

Let's fit a parallel slopes model with the Willett data. You try first.

`r countdown::countdown(2)`

--
```{r }
w0 <- lmer(opp ~ time + (1|id), willett)
```

--
What does the residual variance-covariance look like? Let's use **sundry** to pull it 

```{r }
library(sundry)
w0_rvcv <- pull_residual_vcov(w0)
```

---
# Image
Sparse matrix - we can view it with `image()`

```{r }
image(w0_rvcv)
```

---
# Pull first few rows/cols

```{r }
w0_rvcv[1:8, 1:8]
```

---
class: inverse-green middle
# Next time 
## Modeling growth (part 1)
### Also Homework 2 is assigned
